{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)  # For reproducibility\n",
    "n_series = 4000   # Number of time series per class\n",
    "n_points = 500    # Number of data points in each time series\n",
    "\n",
    "#output_dir = 'time_series_data'\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.expanduser(\"~/timeseries_data\")  # Creates the directory in your home folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR, MA and ARMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate AR, MA, or ARMA data with optional trend and seasonality\n",
    "def generate_time_series(model_type, order, n_points, trend_strength=0.1, seasonality_amplitude=0.5, seasonality_period=50, include_trend=True):\n",
    "   if model_type == 'AR':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -params]\n",
    "       ma = np.array([1])\n",
    "   elif model_type == 'MA':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.array([1])\n",
    "       ma = np.r_[1, params]\n",
    "   elif model_type == 'ARMA':\n",
    "       ar_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ma_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -ar_params]\n",
    "       ma = np.r_[1, ma_params]\n",
    "   else:\n",
    "       raise ValueError(\"Invalid model type. Use 'AR', 'MA', or 'ARMA'.\")\n",
    "   \n",
    "   \n",
    "      # Generate the process\n",
    "   process = sm.tsa.ArmaProcess(ar, ma)\n",
    "   data = process.generate_sample(nsample=n_points)\n",
    "   if include_trend:\n",
    "       trend = np.linspace(0, trend_strength * n_points, n_points)\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += trend + seasonality\n",
    "   else:\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += seasonality\n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for orders 1-3 with and without trend (Kernel crashed after 14 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loop to generate and save time series plots for each class\n",
    "model_types = ['AR', 'MA', 'ARMA']\n",
    "orders = [1, 2, 3]\n",
    "for model_type in model_types:\n",
    "   for order in orders:\n",
    "       for include_trend in [True, False]:\n",
    "           class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "           class_dir = os.path.join(output_dir, class_label)\n",
    "           os.makedirs(class_dir, exist_ok=True)\n",
    "           for i in range(n_series):\n",
    "               data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "               # Plotting the time series\n",
    "               plt.figure(figsize=(8, 4))\n",
    "               plt.plot(data)\n",
    "               plt.axis('off')  # Turn off axes for a clean image\n",
    "               plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "               plt.close()\n",
    "print(\"Time series generation completed. Time series are saved in the 'time_series_data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating image for remaining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMA_1_without_trend: 4000 images\n",
      "ARMA_2_with_trend: 4000 images\n",
      "MA_3_with_trend: 4000 images\n",
      "ARMA_2_without_trend: 4000 images\n",
      "AR_3_without_trend: 4000 images\n",
      "MA_1_without_trend: 4000 images\n",
      "MA_1_with_trend: 4000 images\n",
      "AR_2_with_trend: 4000 images\n",
      "MA_2_without_trend: 4000 images\n",
      "AR_1_with_trend: 4000 images\n",
      "MA_2_with_trend: 4000 images\n",
      "MA_3_without_trend: 4000 images\n",
      "ARMA_3_with_trend: 4000 images\n",
      "AR_3_with_trend: 4000 images\n",
      "AR_2_without_trend: 4000 images\n",
      "ARMA_3_without_trend: 4000 images\n",
      "AR_1_without_trend: 4000 images\n",
      "ARMA_1_with_trend: 4000 images\n"
     ]
    }
   ],
   "source": [
    "# Adjusted classes for ARMA 2 and 3 with and without trend\n",
    "model_type = 'ARMA'\n",
    "orders = [2, 3]\n",
    "for order in orders:\n",
    "    for include_trend in [True, False]:\n",
    "        class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "        class_dir = os.path.join(output_dir, class_label)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for i in range(n_series):\n",
    "            data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "            # Plotting the time series\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(data)\n",
    "            plt.axis('off')  # Turn off axes for a clean image\n",
    "            plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "\n",
    "# Code to check the number of images in each folder\n",
    "folder_status = {}\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_status[folder_name] = len(os.listdir(folder_path))\n",
    "\n",
    "# Display folder status\n",
    "for class_label, image_count in folder_status.items():\n",
    "    print(f\"{class_label}: {image_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Global parameters\n",
    "image_size = (64, 64)  # Resize all images to this size\n",
    "batch_size = 32  # Adjust based on hardware capacity\n",
    "\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")  # Path to your main directory with class subfolders\n",
    "\n",
    "\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])  # Filter directories only\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # Mapping class names to indices\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):  # Ensure it's an image file\n",
    "                img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
    "                img = img.resize(image_size)  # Resize image\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping: {'ARMA_1_with_trend': 0, 'ARMA_1_without_trend': 1, 'ARMA_2_with_trend': 2, 'ARMA_2_without_trend': 3, 'ARMA_3_with_trend': 4, 'ARMA_3_without_trend': 5, 'AR_1_with_trend': 6, 'AR_1_without_trend': 7, 'AR_2_with_trend': 8, 'AR_2_without_trend': 9, 'AR_3_with_trend': 10, 'AR_3_without_trend': 11, 'MA_1_with_trend': 12, 'MA_1_without_trend': 13, 'MA_2_with_trend': 14, 'MA_2_without_trend': 15, 'MA_3_with_trend': 16, 'MA_3_without_trend': 17}\n"
     ]
    }
   ],
   "source": [
    "# Generate a mapping for classes\n",
    "classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # {'AR_1_with_trend': 0, 'MA_1_without_trend': 1, ...}\n",
    "print(f\"Class to index mapping: {class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Classes: ['ARMA_1_with_trend', 'ARMA_1_without_trend', 'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', 'ARMA_3_without_trend', 'AR_1_with_trend', 'AR_1_without_trend', 'AR_2_with_trend', 'AR_2_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_1_with_trend', 'MA_1_without_trend', 'MA_2_with_trend', 'MA_2_without_trend', 'MA_3_with_trend', 'MA_3_without_trend']\n",
      "Number of classes: 18\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Print class information\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Subset of classes to start with (6 classes)\n",
    "subset_classes = classes[:6]\n",
    "subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "\n",
    "# Filter data and labels for the subset\n",
    "subset_mask = np.isin(labels, subset_indices)\n",
    "data_subset = data[subset_mask]\n",
    "labels_subset = labels[subset_mask]\n",
    "\n",
    "# Update labels for the subset (reindex for subset only)\n",
    "labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    data_subset, labels_subset, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split temp into validation (20%) and test (10%)\n",
    "# Temp is already 30%, so we divide it: 20% = (2/3 of temp), 10% = (1/3 of temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=1/3, random_state=42  # 1/3 of temp goes to test set\n",
    ")\n",
    "\n",
    "# Resize the datasets\n",
    "X_train_resized = tf.image.resize(X_train, [224, 224])\n",
    "X_val_resized = tf.image.resize(X_val, [224, 224])\n",
    "X_test_resized = tf.image.resize(X_test, [224, 224])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet 6 classes both 46% ---- > best thus far\n",
    "# with finetuning 78 train but 45 val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 444ms/step - accuracy: 0.3293 - loss: 1.6125 - val_accuracy: 0.4121 - val_loss: 1.3091\n",
      "Epoch 2/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 431ms/step - accuracy: 0.3781 - loss: 1.3616 - val_accuracy: 0.4173 - val_loss: 1.2787\n",
      "Epoch 3/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 426ms/step - accuracy: 0.3882 - loss: 1.3298 - val_accuracy: 0.4465 - val_loss: 1.2474\n",
      "Epoch 4/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 427ms/step - accuracy: 0.4146 - loss: 1.2889 - val_accuracy: 0.4433 - val_loss: 1.2355\n",
      "Epoch 5/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 416ms/step - accuracy: 0.4266 - loss: 1.2750 - val_accuracy: 0.4550 - val_loss: 1.2235\n",
      "Epoch 6/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 426ms/step - accuracy: 0.4293 - loss: 1.2565 - val_accuracy: 0.4577 - val_loss: 1.2133\n",
      "Epoch 7/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 405ms/step - accuracy: 0.4275 - loss: 1.2456 - val_accuracy: 0.4554 - val_loss: 1.2062\n",
      "Epoch 8/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 402ms/step - accuracy: 0.4292 - loss: 1.2316 - val_accuracy: 0.4598 - val_loss: 1.1954\n",
      "Epoch 9/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 409ms/step - accuracy: 0.4407 - loss: 1.2201 - val_accuracy: 0.4563 - val_loss: 1.1983\n",
      "Epoch 10/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 412ms/step - accuracy: 0.4528 - loss: 1.2069 - val_accuracy: 0.4683 - val_loss: 1.1795\n",
      "Epoch 11/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 426ms/step - accuracy: 0.4443 - loss: 1.2065 - val_accuracy: 0.4663 - val_loss: 1.1714\n",
      "Epoch 12/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 441ms/step - accuracy: 0.4580 - loss: 1.1900 - val_accuracy: 0.4760 - val_loss: 1.1655\n",
      "Epoch 13/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 451ms/step - accuracy: 0.4569 - loss: 1.1844 - val_accuracy: 0.4667 - val_loss: 1.1641\n",
      "Epoch 14/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 453ms/step - accuracy: 0.4562 - loss: 1.1745 - val_accuracy: 0.4681 - val_loss: 1.1566\n",
      "Epoch 15/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 425ms/step - accuracy: 0.4650 - loss: 1.1633 - val_accuracy: 0.4608 - val_loss: 1.1574\n",
      "Epoch 16/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 416ms/step - accuracy: 0.4609 - loss: 1.1684 - val_accuracy: 0.4652 - val_loss: 1.1654\n",
      "Epoch 17/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 419ms/step - accuracy: 0.4660 - loss: 1.1506 - val_accuracy: 0.4762 - val_loss: 1.1408\n",
      "Epoch 18/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 412ms/step - accuracy: 0.4733 - loss: 1.1415 - val_accuracy: 0.4702 - val_loss: 1.1486\n",
      "Epoch 19/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 419ms/step - accuracy: 0.4699 - loss: 1.1424 - val_accuracy: 0.4835 - val_loss: 1.1332\n",
      "Epoch 20/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 423ms/step - accuracy: 0.4753 - loss: 1.1358 - val_accuracy: 0.4633 - val_loss: 1.1433\n",
      "Epoch 21/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 425ms/step - accuracy: 0.4624 - loss: 1.1402 - val_accuracy: 0.4762 - val_loss: 1.1239\n",
      "Epoch 22/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 431ms/step - accuracy: 0.4694 - loss: 1.1342 - val_accuracy: 0.4644 - val_loss: 1.1379\n",
      "Epoch 23/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 433ms/step - accuracy: 0.4684 - loss: 1.1329 - val_accuracy: 0.4808 - val_loss: 1.1190\n",
      "Epoch 24/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 421ms/step - accuracy: 0.4811 - loss: 1.1175 - val_accuracy: 0.4837 - val_loss: 1.1159\n",
      "Epoch 25/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 484ms/step - accuracy: 0.4798 - loss: 1.1189 - val_accuracy: 0.4765 - val_loss: 1.1129\n",
      "Epoch 26/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 429ms/step - accuracy: 0.4731 - loss: 1.1144 - val_accuracy: 0.4867 - val_loss: 1.1055\n",
      "Epoch 27/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 430ms/step - accuracy: 0.4881 - loss: 1.1046 - val_accuracy: 0.4796 - val_loss: 1.1085\n",
      "Epoch 28/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 428ms/step - accuracy: 0.4810 - loss: 1.1048 - val_accuracy: 0.4694 - val_loss: 1.1126\n",
      "Epoch 29/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 419ms/step - accuracy: 0.4732 - loss: 1.1071 - val_accuracy: 0.4790 - val_loss: 1.1021\n",
      "Epoch 30/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 433ms/step - accuracy: 0.4884 - loss: 1.0948 - val_accuracy: 0.4819 - val_loss: 1.0962\n",
      "Epoch 31/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 418ms/step - accuracy: 0.4844 - loss: 1.0938 - val_accuracy: 0.4760 - val_loss: 1.0997\n",
      "Epoch 32/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 418ms/step - accuracy: 0.4929 - loss: 1.0892 - val_accuracy: 0.4840 - val_loss: 1.0884\n",
      "Epoch 33/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 427ms/step - accuracy: 0.4921 - loss: 1.0881 - val_accuracy: 0.4815 - val_loss: 1.0898\n",
      "Epoch 34/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 415ms/step - accuracy: 0.4950 - loss: 1.0802 - val_accuracy: 0.4860 - val_loss: 1.0843\n",
      "Epoch 35/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 340ms/step - accuracy: 0.4864 - loss: 1.0830 - val_accuracy: 0.4846 - val_loss: 1.0848\n",
      "75/75 - 13s - 175ms/step - accuracy: 0.4563 - loss: 1.0926\n",
      "Test Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Pooling to reduce feature map dimensions\n",
    "        Dense(1024, activation='swish', kernel_regularizer=l2(1e-4)),  # First dense layer\n",
    "        Dropout(0.6),\n",
    "        Dense(512, activation='swish', kernel_regularizer=l2(1e-4)),  # Second dense layer\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='swish', kernel_regularizer=l2(1e-4)),  # Third dense layer\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='swish', kernel_regularizer=l2(1e-4)),  # Fourth dense layer\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(subset_classes)\n",
    "\n",
    "# Resize the datasets to 224x224\n",
    "X_train_resized = tf.image.resize(X_train, [224, 224])\n",
    "X_val_resized = tf.image.resize(X_val, [224, 224])\n",
    "X_test_resized = tf.image.resize(X_test, [224, 224])\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_mobilenetv2(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train_categorical,  # Use resized and normalized data\n",
    "    validation_data=(X_val_resized, y_val_categorical),  # Use resized and normalized validation data\n",
    "    epochs=35,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test_categorical, verbose=2)  # Use resized and normalized test data\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 352ms/step - accuracy: 0.3957 - loss: 1.5995 - val_accuracy: 0.4021 - val_loss: 1.2811\n",
      "Epoch 2/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 354ms/step - accuracy: 0.4641 - loss: 1.1116 - val_accuracy: 0.4183 - val_loss: 1.2167\n",
      "Epoch 3/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 351ms/step - accuracy: 0.4867 - loss: 1.0875 - val_accuracy: 0.4408 - val_loss: 1.1698\n",
      "Epoch 4/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 352ms/step - accuracy: 0.5034 - loss: 1.0619 - val_accuracy: 0.4421 - val_loss: 1.1819\n",
      "Epoch 5/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 365ms/step - accuracy: 0.5285 - loss: 1.0354 - val_accuracy: 0.4765 - val_loss: 1.1118\n",
      "Epoch 6/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 350ms/step - accuracy: 0.5462 - loss: 1.0049 - val_accuracy: 0.4812 - val_loss: 1.1102\n",
      "Epoch 7/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 344ms/step - accuracy: 0.5571 - loss: 0.9886 - val_accuracy: 0.4829 - val_loss: 1.1271\n",
      "Epoch 8/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 349ms/step - accuracy: 0.5729 - loss: 0.9617 - val_accuracy: 0.4781 - val_loss: 1.1254\n",
      "Epoch 9/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 354ms/step - accuracy: 0.5911 - loss: 0.9315 - val_accuracy: 0.4790 - val_loss: 1.1665\n",
      "Epoch 10/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 358ms/step - accuracy: 0.6132 - loss: 0.8982 - val_accuracy: 0.4719 - val_loss: 1.2325\n",
      "Epoch 11/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 355ms/step - accuracy: 0.6387 - loss: 0.8637 - val_accuracy: 0.4744 - val_loss: 1.2619\n",
      "Epoch 12/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 355ms/step - accuracy: 0.6598 - loss: 0.8275 - val_accuracy: 0.4896 - val_loss: 1.2174\n",
      "Epoch 13/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 354ms/step - accuracy: 0.6857 - loss: 0.7917 - val_accuracy: 0.4781 - val_loss: 1.2885\n",
      "Epoch 14/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 354ms/step - accuracy: 0.6950 - loss: 0.7616 - val_accuracy: 0.4806 - val_loss: 1.3043\n",
      "Epoch 15/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 355ms/step - accuracy: 0.7216 - loss: 0.7214 - val_accuracy: 0.4585 - val_loss: 1.6098\n",
      "Epoch 16/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 356ms/step - accuracy: 0.7404 - loss: 0.6879 - val_accuracy: 0.4600 - val_loss: 1.6026\n",
      "Epoch 17/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 353ms/step - accuracy: 0.7467 - loss: 0.6663 - val_accuracy: 0.4663 - val_loss: 1.5424\n",
      "Epoch 18/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 354ms/step - accuracy: 0.7716 - loss: 0.6328 - val_accuracy: 0.4650 - val_loss: 1.6059\n",
      "Epoch 19/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 360ms/step - accuracy: 0.7743 - loss: 0.6149 - val_accuracy: 0.4610 - val_loss: 1.6680\n",
      "Epoch 20/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 357ms/step - accuracy: 0.7879 - loss: 0.5907 - val_accuracy: 0.4583 - val_loss: 1.7331\n"
     ]
    }
   ],
   "source": [
    "base_model = model.layers[0]  # Access the base MobileNetV2 model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Optionally freeze earlier layers for stability\n",
    "for layer in base_model.layers[:100]:  # Adjust based on experimentation\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "history_fine_tune = model.fit(\n",
    "    X_train_resized, y_train_categorical,\n",
    "    validation_data=(X_val_resized, y_val_categorical),\n",
    "    epochs=20,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 181ms/step - accuracy: 0.4629 - loss: 1.7297\n",
      "Test Loss: 1.7349\n",
      "Test Accuracy: 45.83%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test_categorical, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same as above but other architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 239ms/step - accuracy: 0.3237 - loss: 1.7765 - val_accuracy: 0.3754 - val_loss: 1.5090\n",
      "Epoch 2/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 236ms/step - accuracy: 0.3628 - loss: 1.5269 - val_accuracy: 0.3821 - val_loss: 1.4668\n",
      "Epoch 3/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 225ms/step - accuracy: 0.3833 - loss: 1.4789 - val_accuracy: 0.3967 - val_loss: 1.4368\n",
      "Epoch 4/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 229ms/step - accuracy: 0.4024 - loss: 1.4440 - val_accuracy: 0.4271 - val_loss: 1.4008\n",
      "Epoch 5/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 423ms/step - accuracy: 0.4175 - loss: 1.4132 - val_accuracy: 0.4406 - val_loss: 1.3743\n",
      "Epoch 6/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 402ms/step - accuracy: 0.4147 - loss: 1.3932 - val_accuracy: 0.4383 - val_loss: 1.3543\n",
      "Epoch 7/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 441ms/step - accuracy: 0.4364 - loss: 1.3662 - val_accuracy: 0.4465 - val_loss: 1.3337\n",
      "Epoch 8/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 434ms/step - accuracy: 0.4417 - loss: 1.3434 - val_accuracy: 0.4594 - val_loss: 1.3091\n",
      "Epoch 9/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 430ms/step - accuracy: 0.4447 - loss: 1.3248 - val_accuracy: 0.4354 - val_loss: 1.3200\n",
      "Epoch 10/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 433ms/step - accuracy: 0.4412 - loss: 1.3134 - val_accuracy: 0.4548 - val_loss: 1.2849\n",
      "Epoch 11/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 436ms/step - accuracy: 0.4463 - loss: 1.2930 - val_accuracy: 0.4519 - val_loss: 1.2717\n",
      "Epoch 12/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 333ms/step - accuracy: 0.4570 - loss: 1.2752 - val_accuracy: 0.4346 - val_loss: 1.2940\n",
      "Epoch 13/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 236ms/step - accuracy: 0.4451 - loss: 1.2696 - val_accuracy: 0.4644 - val_loss: 1.2431\n",
      "Epoch 14/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 376ms/step - accuracy: 0.4602 - loss: 1.2509 - val_accuracy: 0.4517 - val_loss: 1.2358\n",
      "Epoch 15/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 429ms/step - accuracy: 0.4543 - loss: 1.2381 - val_accuracy: 0.4719 - val_loss: 1.2193\n",
      "Epoch 16/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 442ms/step - accuracy: 0.4603 - loss: 1.2270 - val_accuracy: 0.4648 - val_loss: 1.2091\n",
      "Epoch 17/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 437ms/step - accuracy: 0.4610 - loss: 1.2150 - val_accuracy: 0.4660 - val_loss: 1.2043\n",
      "Epoch 18/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 418ms/step - accuracy: 0.4666 - loss: 1.2026 - val_accuracy: 0.4648 - val_loss: 1.1909\n",
      "Epoch 19/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 419ms/step - accuracy: 0.4687 - loss: 1.1968 - val_accuracy: 0.4685 - val_loss: 1.1880\n",
      "Epoch 20/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 425ms/step - accuracy: 0.4705 - loss: 1.1874 - val_accuracy: 0.4765 - val_loss: 1.1730\n",
      "Epoch 21/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 425ms/step - accuracy: 0.4653 - loss: 1.1813 - val_accuracy: 0.4558 - val_loss: 1.1760\n",
      "Epoch 22/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 421ms/step - accuracy: 0.4626 - loss: 1.1753 - val_accuracy: 0.4690 - val_loss: 1.1710\n",
      "Epoch 23/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 427ms/step - accuracy: 0.4732 - loss: 1.1667 - val_accuracy: 0.4719 - val_loss: 1.1621\n",
      "Epoch 24/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 430ms/step - accuracy: 0.4725 - loss: 1.1566 - val_accuracy: 0.4685 - val_loss: 1.1526\n",
      "Epoch 25/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 429ms/step - accuracy: 0.4739 - loss: 1.1437 - val_accuracy: 0.4638 - val_loss: 1.1591\n",
      "Epoch 26/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 432ms/step - accuracy: 0.4790 - loss: 1.1460 - val_accuracy: 0.4760 - val_loss: 1.1423\n",
      "Epoch 27/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 427ms/step - accuracy: 0.4763 - loss: 1.1364 - val_accuracy: 0.4750 - val_loss: 1.1337\n",
      "Epoch 28/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 433ms/step - accuracy: 0.4822 - loss: 1.1320 - val_accuracy: 0.4665 - val_loss: 1.1362\n",
      "Epoch 29/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 438ms/step - accuracy: 0.4796 - loss: 1.1281 - val_accuracy: 0.4679 - val_loss: 1.1313\n",
      "Epoch 30/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 430ms/step - accuracy: 0.4766 - loss: 1.1237 - val_accuracy: 0.4762 - val_loss: 1.1254\n",
      "Epoch 31/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 421ms/step - accuracy: 0.4845 - loss: 1.1186 - val_accuracy: 0.4683 - val_loss: 1.1229\n",
      "Epoch 32/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 416ms/step - accuracy: 0.4789 - loss: 1.1124 - val_accuracy: 0.4683 - val_loss: 1.1196\n",
      "Epoch 33/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 418ms/step - accuracy: 0.4887 - loss: 1.1008 - val_accuracy: 0.4787 - val_loss: 1.1061\n",
      "Epoch 34/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 418ms/step - accuracy: 0.4884 - loss: 1.0995 - val_accuracy: 0.4785 - val_loss: 1.1075\n",
      "Epoch 35/35\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 420ms/step - accuracy: 0.4784 - loss: 1.0996 - val_accuracy: 0.4746 - val_loss: 1.1012\n",
      "75/75 - 24s - 319ms/step - accuracy: 0.4658 - loss: 1.1092\n",
      "Test Accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Pooling to reduce feature map dimensions\n",
    "        Dense(1024, activation='swish', kernel_regularizer=l2(1e-4)),  # First dense layer\n",
    "        Dropout(0.6),\n",
    "        Dense(512, activation='swish', kernel_regularizer=l2(1e-4)),  # Second dense layer\n",
    "        Dropout(0.6),\n",
    "        Dense(256, activation='swish', kernel_regularizer=l2(1e-4)),  # Third dense layer\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='swish', kernel_regularizer=l2(1e-4)),  # Fourth dense layer\n",
    "        Dropout(0.4),\n",
    "        Dense(512, activation='swish', kernel_regularizer=l2(1e-3)),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "        \n",
    "\n",
    "])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(subset_classes)\n",
    "\n",
    "# Resize the datasets to 224x224\n",
    "X_train_resized = tf.image.resize(X_train, [224, 224])\n",
    "X_val_resized = tf.image.resize(X_val, [224, 224])\n",
    "X_test_resized = tf.image.resize(X_test, [224, 224])\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_mobilenetv2(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train_categorical,  # Use resized and normalized data\n",
    "    validation_data=(X_val_resized, y_val_categorical),  # Use resized and normalized validation data\n",
    "    epochs=35,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_resized, y_test_categorical, verbose=2)  # Use resized and normalized test data\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 462ms/step - accuracy: 0.4356 - loss: 1.3400 - val_accuracy: 0.4208 - val_loss: 1.1830\n",
      "Epoch 2/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 456ms/step - accuracy: 0.4833 - loss: 1.0981 - val_accuracy: 0.4613 - val_loss: 1.1228\n",
      "Epoch 3/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 449ms/step - accuracy: 0.4824 - loss: 1.0907 - val_accuracy: 0.4623 - val_loss: 1.1206\n",
      "Epoch 4/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 448ms/step - accuracy: 0.4846 - loss: 1.0894 - val_accuracy: 0.4812 - val_loss: 1.0893\n",
      "Epoch 5/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 668ms/step - accuracy: 0.4938 - loss: 1.0853 - val_accuracy: 0.4896 - val_loss: 1.0888\n",
      "Epoch 6/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 668ms/step - accuracy: 0.5018 - loss: 1.0711 - val_accuracy: 0.4812 - val_loss: 1.0974\n",
      "Epoch 7/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 669ms/step - accuracy: 0.5026 - loss: 1.0760 - val_accuracy: 0.4837 - val_loss: 1.0881\n",
      "Epoch 8/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 665ms/step - accuracy: 0.5048 - loss: 1.0674 - val_accuracy: 0.4856 - val_loss: 1.0883\n",
      "Epoch 9/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 667ms/step - accuracy: 0.5064 - loss: 1.0669 - val_accuracy: 0.4902 - val_loss: 1.0853\n",
      "Epoch 10/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 671ms/step - accuracy: 0.5131 - loss: 1.0621 - val_accuracy: 0.4900 - val_loss: 1.0852\n",
      "Epoch 11/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 689ms/step - accuracy: 0.5119 - loss: 1.0569 - val_accuracy: 0.4879 - val_loss: 1.0864\n",
      "Epoch 12/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 675ms/step - accuracy: 0.5186 - loss: 1.0561 - val_accuracy: 0.4910 - val_loss: 1.0812\n",
      "Epoch 13/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 669ms/step - accuracy: 0.5080 - loss: 1.0560 - val_accuracy: 0.4935 - val_loss: 1.0835\n",
      "Epoch 14/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 675ms/step - accuracy: 0.5209 - loss: 1.0458 - val_accuracy: 0.4885 - val_loss: 1.0872\n",
      "Epoch 15/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 668ms/step - accuracy: 0.5276 - loss: 1.0449 - val_accuracy: 0.4927 - val_loss: 1.0819\n",
      "Epoch 16/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 671ms/step - accuracy: 0.5264 - loss: 1.0351 - val_accuracy: 0.4946 - val_loss: 1.0818\n",
      "Epoch 17/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 679ms/step - accuracy: 0.5229 - loss: 1.0386 - val_accuracy: 0.4956 - val_loss: 1.0798\n",
      "Epoch 18/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 737ms/step - accuracy: 0.5245 - loss: 1.0431 - val_accuracy: 0.4913 - val_loss: 1.0840\n",
      "Epoch 19/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 680ms/step - accuracy: 0.5299 - loss: 1.0323 - val_accuracy: 0.4875 - val_loss: 1.0879\n",
      "Epoch 20/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 683ms/step - accuracy: 0.5286 - loss: 1.0369 - val_accuracy: 0.4898 - val_loss: 1.0869\n"
     ]
    }
   ],
   "source": [
    "base_model = model.layers[0]  # Access the base MobileNetV2 model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Optionally freeze earlier layers for stability\n",
    "for layer in base_model.layers[:150]:  # Adjust based on experimentation\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "history_fine_tune = model.fit(\n",
    "    X_train_resized, y_train_categorical,\n",
    "    validation_data=(X_val_resized, y_val_categorical),\n",
    "    epochs=20,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 690ms/step - accuracy: 0.5265 - loss: 1.0331 - val_accuracy: 0.4906 - val_loss: 1.0826\n",
      "Epoch 2/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 680ms/step - accuracy: 0.5408 - loss: 1.0211 - val_accuracy: 0.4904 - val_loss: 1.0828\n",
      "Epoch 3/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 675ms/step - accuracy: 0.5374 - loss: 1.0181 - val_accuracy: 0.4938 - val_loss: 1.0844\n",
      "Epoch 4/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 629ms/step - accuracy: 0.5349 - loss: 1.0202 - val_accuracy: 0.4942 - val_loss: 1.0856\n",
      "Epoch 5/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 464ms/step - accuracy: 0.5474 - loss: 1.0102 - val_accuracy: 0.4965 - val_loss: 1.0815\n",
      "Epoch 6/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 461ms/step - accuracy: 0.5498 - loss: 1.0020 - val_accuracy: 0.4915 - val_loss: 1.0832\n",
      "Epoch 7/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 644ms/step - accuracy: 0.5472 - loss: 1.0051 - val_accuracy: 0.4917 - val_loss: 1.0858\n",
      "Epoch 8/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 689ms/step - accuracy: 0.5427 - loss: 1.0061 - val_accuracy: 0.4910 - val_loss: 1.0907\n",
      "Epoch 9/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 690ms/step - accuracy: 0.5485 - loss: 1.0017 - val_accuracy: 0.4848 - val_loss: 1.1043\n",
      "Epoch 10/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 695ms/step - accuracy: 0.5498 - loss: 0.9995 - val_accuracy: 0.4931 - val_loss: 1.0873\n",
      "Epoch 11/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 700ms/step - accuracy: 0.5516 - loss: 0.9938 - val_accuracy: 0.4837 - val_loss: 1.0933\n",
      "Epoch 12/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 697ms/step - accuracy: 0.5577 - loss: 0.9888 - val_accuracy: 0.4883 - val_loss: 1.0975\n",
      "Epoch 13/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 684ms/step - accuracy: 0.5607 - loss: 0.9888 - val_accuracy: 0.4979 - val_loss: 1.0874\n",
      "Epoch 14/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 679ms/step - accuracy: 0.5540 - loss: 0.9884 - val_accuracy: 0.4885 - val_loss: 1.0998\n",
      "Epoch 15/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 680ms/step - accuracy: 0.5500 - loss: 0.9884 - val_accuracy: 0.4933 - val_loss: 1.0988\n",
      "Epoch 16/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 523ms/step - accuracy: 0.5676 - loss: 0.9766 - val_accuracy: 0.4852 - val_loss: 1.1094\n",
      "Epoch 17/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 450ms/step - accuracy: 0.5587 - loss: 0.9854 - val_accuracy: 0.4948 - val_loss: 1.0931\n",
      "Epoch 18/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 456ms/step - accuracy: 0.5612 - loss: 0.9766 - val_accuracy: 0.4900 - val_loss: 1.1003\n",
      "Epoch 19/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 452ms/step - accuracy: 0.5691 - loss: 0.9726 - val_accuracy: 0.4931 - val_loss: 1.0969\n",
      "Epoch 20/20\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 452ms/step - accuracy: 0.5731 - loss: 0.9621 - val_accuracy: 0.4956 - val_loss: 1.0984\n"
     ]
    }
   ],
   "source": [
    "base_model = model.layers[0]  # Access the base MobileNetV2 model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Optionally freeze earlier layers for stability\n",
    "for layer in base_model.layers[:110]:  # Adjust based on experimentation\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "history_fine_tune = model.fit(\n",
    "    X_train_resized, y_train_categorical,\n",
    "    validation_data=(X_val_resized, y_val_categorical),\n",
    "    epochs=20,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,322</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │         \u001b[38;5;34m2,322\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,384,402</span> (100.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,384,402\u001b[0m (100.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,793,106</span> (10.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,793,106\u001b[0m (10.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,591,296</span> (89.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,591,296\u001b[0m (89.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m  11/1800\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:00:59\u001b[0m 2s/step - accuracy: 0.0505 - loss: 4.0104"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     63\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val_resized, y_val)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_complex_model(input_shape, num_classes):\n",
    "    # Load the ResNet50 model pre-trained on ImageNet\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model layers to prevent training initially\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add a more complex classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(1e-4)),  # Increased units\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')  # Final layer for classification\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_shape = (224, 224, 3)  # RGB images resized to 224x224\n",
    "num_classes = 18  # Number of classes\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_complex_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val_resized, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Fine-tune the base model\n",
    "base_model = model.layers[0]  # Access the base model (ResNet50)\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first few layers for stability\n",
    "for layer in base_model.layers[:100]:  # Adjust based on experimentation\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,079,241</span> (11.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,079,241\u001b[0m (11.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">821,257</span> (3.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m821,257\u001b[0m (3.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 74/900\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:43\u001b[0m 343ms/step - accuracy: 0.1713 - loss: 2.3207"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def build_transfer_model(input_shape, num_classes):\n",
    "    # Load the MobileNetV2 model pre-trained on ImageNet\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model layers to prevent training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom classification head\n",
    "    model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.6),\n",
    "    Dense(256, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='swish', kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "#image_size = (224, 224)  # Resize all images to this size\n",
    "#input_shape = (image_size[0], image_size[1], 3)  # RGB images\n",
    "\n",
    "\n",
    "# Parameters\n",
    "image_size = (224, 224)\n",
    "input_shape = (image_size[0], image_size[1], 3)  # RGB images\n",
    "num_classes = len(subset_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_transfer_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model = model.layers[0]  # Access the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first few layers for stability (optional)\n",
    "for layer in base_model.layers[:50]:  # Adjust the number of layers as needed\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,  # Fewer epochs for fine-tuning\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 50% val 35% with finetune 1/2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_7      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,161</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_7      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m1,161\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,079,241</span> (11.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,079,241\u001b[0m (11.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">821,257</span> (3.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m821,257\u001b[0m (3.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 223ms/step - accuracy: 0.2313 - loss: 1.7523 - val_accuracy: 0.2826 - val_loss: 1.4767 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 224ms/step - accuracy: 0.2706 - loss: 1.5338 - val_accuracy: 0.3044 - val_loss: 1.4470 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 224ms/step - accuracy: 0.2914 - loss: 1.4881 - val_accuracy: 0.3179 - val_loss: 1.4202 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 223ms/step - accuracy: 0.3056 - loss: 1.4623 - val_accuracy: 0.3242 - val_loss: 1.4116 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 224ms/step - accuracy: 0.3119 - loss: 1.4496 - val_accuracy: 0.3367 - val_loss: 1.4030 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 225ms/step - accuracy: 0.3201 - loss: 1.4365 - val_accuracy: 0.3325 - val_loss: 1.3990 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3211 - loss: 1.4277 - val_accuracy: 0.3363 - val_loss: 1.3940 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3245 - loss: 1.4188 - val_accuracy: 0.3328 - val_loss: 1.3956 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3260 - loss: 1.4151 - val_accuracy: 0.3378 - val_loss: 1.3836 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3298 - loss: 1.4031 - val_accuracy: 0.3343 - val_loss: 1.3799 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 228ms/step - accuracy: 0.3377 - loss: 1.3966 - val_accuracy: 0.3442 - val_loss: 1.3758 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3349 - loss: 1.3920 - val_accuracy: 0.3465 - val_loss: 1.3756 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3425 - loss: 1.3881 - val_accuracy: 0.3442 - val_loss: 1.3703 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 231ms/step - accuracy: 0.3400 - loss: 1.3874 - val_accuracy: 0.3468 - val_loss: 1.3720 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 230ms/step - accuracy: 0.3464 - loss: 1.3814 - val_accuracy: 0.3392 - val_loss: 1.3734 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 230ms/step - accuracy: 0.3407 - loss: 1.3764 - val_accuracy: 0.3492 - val_loss: 1.3718 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3445 - loss: 1.3778 - val_accuracy: 0.3426 - val_loss: 1.3745 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 228ms/step - accuracy: 0.3534 - loss: 1.3739 - val_accuracy: 0.3467 - val_loss: 1.3661 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3539 - loss: 1.3656 - val_accuracy: 0.3461 - val_loss: 1.3632 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 229ms/step - accuracy: 0.3498 - loss: 1.3623 - val_accuracy: 0.3507 - val_loss: 1.3627 - learning_rate: 1.0000e-04\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2209 - loss: 8.4535\n",
      "Validation Accuracy: 22.22%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def build_transfer_model(input_shape, num_classes):\n",
    "    # Load the MobileNetV2 model pre-trained on ImageNet\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model layers to prevent training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom classification head\n",
    "    model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Reduce the spatial dimensions\n",
    "    Dense(512, activation='swish'),  # First dense layer with more units\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(256, activation='swish'),  # Second dense layer\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(128, activation='swish'),  # Third dense layer\n",
    "    Dropout(0.3),  # Additional dropout\n",
    "    Dense(num_classes, activation='softmax')  # Final output layer\n",
    "])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#image_size = (224, 224)  # Resize all images to this size\n",
    "#input_shape = (image_size[0], image_size[1], 3)  # RGB images\n",
    "\n",
    "\n",
    "# Parameters\n",
    "image_size = (224, 224)\n",
    "input_shape = (image_size[0], image_size[1], 3)  # RGB images\n",
    "num_classes = len(subset_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_transfer_model(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 354ms/step - accuracy: 0.2581 - loss: 2.4184 - val_accuracy: 0.2883 - val_loss: 1.6000 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 348ms/step - accuracy: 0.3257 - loss: 1.4107 - val_accuracy: 0.3060 - val_loss: 1.4883 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 365ms/step - accuracy: 0.3482 - loss: 1.3736 - val_accuracy: 0.3392 - val_loss: 1.3785 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 354ms/step - accuracy: 0.3633 - loss: 1.3412 - val_accuracy: 0.3422 - val_loss: 1.3646 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 363ms/step - accuracy: 0.3741 - loss: 1.3198 - val_accuracy: 0.3564 - val_loss: 1.3487 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 429ms/step - accuracy: 0.3880 - loss: 1.2945 - val_accuracy: 0.3536 - val_loss: 1.3618 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 379ms/step - accuracy: 0.3965 - loss: 1.2755 - val_accuracy: 0.3624 - val_loss: 1.3499 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 355ms/step - accuracy: 0.4095 - loss: 1.2500 - val_accuracy: 0.3587 - val_loss: 1.3595 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 646ms/step - accuracy: 0.4309 - loss: 1.2258 - val_accuracy: 0.3613 - val_loss: 1.3732 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 351ms/step - accuracy: 0.4422 - loss: 1.2018 - val_accuracy: 0.3550 - val_loss: 1.4116 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 354ms/step - accuracy: 0.4546 - loss: 1.1691 - val_accuracy: 0.3571 - val_loss: 1.3984 - learning_rate: 5.0000e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 353ms/step - accuracy: 0.4717 - loss: 1.1438 - val_accuracy: 0.3603 - val_loss: 1.3985 - learning_rate: 5.0000e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 354ms/step - accuracy: 0.4721 - loss: 1.1305 - val_accuracy: 0.3617 - val_loss: 1.4177 - learning_rate: 5.0000e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 351ms/step - accuracy: 0.4774 - loss: 1.1174 - val_accuracy: 0.3481 - val_loss: 1.4639 - learning_rate: 5.0000e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 352ms/step - accuracy: 0.4866 - loss: 1.1041 - val_accuracy: 0.3582 - val_loss: 1.4476 - learning_rate: 5.0000e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 352ms/step - accuracy: 0.5043 - loss: 1.0710 - val_accuracy: 0.3554 - val_loss: 1.4550 - learning_rate: 2.5000e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 346ms/step - accuracy: 0.5056 - loss: 1.0698 - val_accuracy: 0.3564 - val_loss: 1.4865 - learning_rate: 2.5000e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 346ms/step - accuracy: 0.5007 - loss: 1.0678 - val_accuracy: 0.3567 - val_loss: 1.4804 - learning_rate: 2.5000e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 355ms/step - accuracy: 0.5131 - loss: 1.0523 - val_accuracy: 0.3560 - val_loss: 1.4888 - learning_rate: 2.5000e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 361ms/step - accuracy: 0.5262 - loss: 1.0360 - val_accuracy: 0.3540 - val_loss: 1.5128 - learning_rate: 2.5000e-06\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the base model\n",
    "base_model = model.layers[0]  # Access the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first few layers for stability (optional)\n",
    "for layer in base_model.layers[:100]:  # Adjust the number of layers as needed\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    X_train_resized, y_train,\n",
    "    validation_data=(X_val_resized, y_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,  # Fewer epochs for fine-tuning\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset splitting without hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess only 8 classes for training\n",
    "def filter_classes(data, labels, classes, subset_size=10):\n",
    "    \"\"\"Filters data and labels to only include a subset of classes.\"\"\"\n",
    "    selected_classes = classes[:subset_size]\n",
    "    class_indices = [classes.index(cls) for cls in selected_classes]\n",
    "    filtered_data = []\n",
    "    filtered_labels = []\n",
    "    for img, lbl in zip(data, labels):\n",
    "        if lbl in class_indices:\n",
    "            filtered_data.append(img)\n",
    "            filtered_labels.append(lbl)\n",
    "    return np.array(filtered_data), np.array(filtered_labels), selected_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your existing code to load the dataset\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Filter the first 8 classes for training\n",
    "subset_size = 10\n",
    "filtered_data, filtered_labels, selected_classes = filter_classes(data, labels, classes, subset_size=subset_size)\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "filtered_data, filtered_labels = shuffle(filtered_data, filtered_labels, random_state=123)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(filtered_data, filtered_labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert numerical labels to one-hot encoding\n",
    "if len(y_train.shape) == 1:\n",
    "    y_train = to_categorical(y_train, num_classes=len(selected_classes))\n",
    "    y_val = to_categorical(y_val, num_classes=len(selected_classes))\n",
    "    y_test = to_categorical(y_test, num_classes=len(selected_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 182ms/step - accuracy: 0.1558 - loss: 14.8425 - val_accuracy: 0.1972 - val_loss: 4.2489 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.1994 - loss: 3.6785 - val_accuracy: 0.2033 - val_loss: 2.6674 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2034 - loss: 2.5492 - val_accuracy: 0.2069 - val_loss: 2.1977 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 177ms/step - accuracy: 0.1977 - loss: 2.1480 - val_accuracy: 0.2035 - val_loss: 1.9631 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2044 - loss: 1.9366 - val_accuracy: 0.2085 - val_loss: 1.8336 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2061 - loss: 1.8244 - val_accuracy: 0.2140 - val_loss: 1.7650 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.2000 - loss: 1.7639 - val_accuracy: 0.2085 - val_loss: 1.7281 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 174ms/step - accuracy: 0.2002 - loss: 1.7326 - val_accuracy: 0.2052 - val_loss: 1.7071 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2025 - loss: 1.7114 - val_accuracy: 0.2145 - val_loss: 1.6958 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 177ms/step - accuracy: 0.2087 - loss: 1.6975 - val_accuracy: 0.2045 - val_loss: 1.6866 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 177ms/step - accuracy: 0.2038 - loss: 1.6936 - val_accuracy: 0.1982 - val_loss: 1.6890 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 177ms/step - accuracy: 0.2012 - loss: 1.6880 - val_accuracy: 0.2114 - val_loss: 1.6772 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.2030 - loss: 1.6824 - val_accuracy: 0.2116 - val_loss: 1.6735 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 174ms/step - accuracy: 0.2044 - loss: 1.6788 - val_accuracy: 0.2104 - val_loss: 1.6712 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 175ms/step - accuracy: 0.2024 - loss: 1.6761 - val_accuracy: 0.2060 - val_loss: 1.6684 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 176ms/step - accuracy: 0.2071 - loss: 1.6718 - val_accuracy: 0.2099 - val_loss: 1.6654 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 182ms/step - accuracy: 0.2069 - loss: 1.6694 - val_accuracy: 0.2107 - val_loss: 1.6630 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 188ms/step - accuracy: 0.2024 - loss: 1.6689 - val_accuracy: 0.2094 - val_loss: 1.6626 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 186ms/step - accuracy: 0.2054 - loss: 1.6665 - val_accuracy: 0.2126 - val_loss: 1.6599 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 187ms/step - accuracy: 0.2085 - loss: 1.6644 - val_accuracy: 0.2039 - val_loss: 1.6592 - learning_rate: 1.0000e-04\n",
      "Epoch 1/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 439ms/step - accuracy: 0.2114 - loss: 1.6711 - val_accuracy: 0.2066 - val_loss: 1.6596 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 437ms/step - accuracy: 0.2340 - loss: 1.6126 - val_accuracy: 0.2064 - val_loss: 1.6514 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 430ms/step - accuracy: 0.2494 - loss: 1.5948 - val_accuracy: 0.2066 - val_loss: 1.6747 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 431ms/step - accuracy: 0.2532 - loss: 1.5869 - val_accuracy: 0.2447 - val_loss: 1.6039 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 421ms/step - accuracy: 0.2564 - loss: 1.5789 - val_accuracy: 0.1996 - val_loss: 2.4073 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 425ms/step - accuracy: 0.2618 - loss: 1.5747 - val_accuracy: 0.1994 - val_loss: 2.1046 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.2590 - loss: 1.5710\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 425ms/step - accuracy: 0.2590 - loss: 1.5710 - val_accuracy: 0.2229 - val_loss: 1.6322 - learning_rate: 1.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 427ms/step - accuracy: 0.2651 - loss: 1.5661 - val_accuracy: 0.2670 - val_loss: 1.5664 - learning_rate: 5.0000e-06\n",
      "Epoch 9/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2715 - loss: 1.5578 - val_accuracy: 0.2256 - val_loss: 1.6825 - learning_rate: 5.0000e-06\n",
      "Epoch 10/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2683 - loss: 1.5595 - val_accuracy: 0.2676 - val_loss: 1.5833 - learning_rate: 5.0000e-06\n",
      "Epoch 11/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.2742 - loss: 1.5531\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2742 - loss: 1.5531 - val_accuracy: 0.2290 - val_loss: 1.7029 - learning_rate: 5.0000e-06\n",
      "Epoch 12/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 424ms/step - accuracy: 0.2732 - loss: 1.5497 - val_accuracy: 0.2495 - val_loss: 1.5957 - learning_rate: 2.5000e-06\n",
      "Epoch 13/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 423ms/step - accuracy: 0.2863 - loss: 1.5417 - val_accuracy: 0.2766 - val_loss: 1.5700 - learning_rate: 2.5000e-06\n",
      "125/125 - 10s - 81ms/step - accuracy: 0.2862 - loss: 1.5626\n",
      "Test Accuracy: 0.29\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Number of classes after filtering\n",
    "num_classes = len(selected_classes)  # Should be 10\n",
    "\n",
    "# Define the ResNet50 model\n",
    "def build_resnet50(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Reduces feature map dimensions\n",
    "        Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),  # Regularization\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer dynamically set to 10 classes\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (64, 64, 3)\n",
    "model = build_resnet50(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for learning rate adjustment and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune the base model\n",
    "base_model = model.layers[0]  # Extract base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Unfreeze the last 50 layers for fine-tuning\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subset 10 classes accuracy 31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/b81lcpw16vngs5l3k2jfppy00000gn/T/ipykernel_6120/755212799.py:13: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.2115 - loss: 64.6376 - val_accuracy: 0.2599 - val_loss: 16.8409 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 44ms/step - accuracy: 0.2435 - loss: 12.2803 - val_accuracy: 0.2627 - val_loss: 4.6902 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 44ms/step - accuracy: 0.2476 - loss: 3.9044 - val_accuracy: 0.2666 - val_loss: 2.3987 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2507 - loss: 2.2338 - val_accuracy: 0.2460 - val_loss: 1.8483 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2481 - loss: 1.8182 - val_accuracy: 0.2554 - val_loss: 1.6946 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2500 - loss: 1.7049 - val_accuracy: 0.2609 - val_loss: 1.6602 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2532 - loss: 1.6678 - val_accuracy: 0.2624 - val_loss: 1.6359 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.2499 - loss: 1.6524 - val_accuracy: 0.2515 - val_loss: 1.6308 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.2519 - loss: 1.6458 - val_accuracy: 0.2573 - val_loss: 1.6270 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2484 - loss: 1.6412 - val_accuracy: 0.2426 - val_loss: 1.6272 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2548 - loss: 1.6355 - val_accuracy: 0.2639 - val_loss: 1.6187 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2514 - loss: 1.6294 - val_accuracy: 0.2716 - val_loss: 1.6149 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2590 - loss: 1.6250 - val_accuracy: 0.2696 - val_loss: 1.6121 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2588 - loss: 1.6202 - val_accuracy: 0.2612 - val_loss: 1.6093 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2608 - loss: 1.6176 - val_accuracy: 0.2654 - val_loss: 1.6055 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2603 - loss: 1.6165 - val_accuracy: 0.2699 - val_loss: 1.6034 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2560 - loss: 1.6172 - val_accuracy: 0.2627 - val_loss: 1.6020 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.2586 - loss: 1.6101 - val_accuracy: 0.2686 - val_loss: 1.6027 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2612 - loss: 1.6093 - val_accuracy: 0.2592 - val_loss: 1.5991 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.2666 - loss: 1.6058 - val_accuracy: 0.2652 - val_loss: 1.6003 - learning_rate: 1.0000e-04\n",
      "Epoch 1/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 74ms/step - accuracy: 0.2234 - loss: 1.8132 - val_accuracy: 0.2589 - val_loss: 1.6438 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 71ms/step - accuracy: 0.2755 - loss: 1.6055 - val_accuracy: 0.2688 - val_loss: 1.6379 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 72ms/step - accuracy: 0.2824 - loss: 1.5873 - val_accuracy: 0.2791 - val_loss: 1.6253 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 73ms/step - accuracy: 0.3031 - loss: 1.5589 - val_accuracy: 0.2856 - val_loss: 1.5985 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3057 - loss: 1.5511 - val_accuracy: 0.2944 - val_loss: 1.5752 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3122 - loss: 1.5381 - val_accuracy: 0.3009 - val_loss: 1.5563 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 73ms/step - accuracy: 0.3175 - loss: 1.5283 - val_accuracy: 0.3036 - val_loss: 1.5417 - learning_rate: 1.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3257 - loss: 1.5141 - val_accuracy: 0.3030 - val_loss: 1.5402 - learning_rate: 1.0000e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3249 - loss: 1.5077 - val_accuracy: 0.3085 - val_loss: 1.5329 - learning_rate: 1.0000e-05\n",
      "Epoch 10/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 73ms/step - accuracy: 0.3304 - loss: 1.4985 - val_accuracy: 0.3084 - val_loss: 1.5323 - learning_rate: 1.0000e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 73ms/step - accuracy: 0.3389 - loss: 1.4847 - val_accuracy: 0.3047 - val_loss: 1.5479 - learning_rate: 1.0000e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3501 - loss: 1.4796 - val_accuracy: 0.3088 - val_loss: 1.5389 - learning_rate: 1.0000e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m437/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3534 - loss: 1.4680\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3534 - loss: 1.4680 - val_accuracy: 0.3050 - val_loss: 1.5413 - learning_rate: 1.0000e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3536 - loss: 1.4590 - val_accuracy: 0.3056 - val_loss: 1.5552 - learning_rate: 5.0000e-06\n",
      "Epoch 15/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.3605 - loss: 1.4455 - val_accuracy: 0.3030 - val_loss: 1.5574 - learning_rate: 5.0000e-06\n",
      "125/125 - 3s - 25ms/step - accuracy: 0.3110 - loss: 1.5276\n",
      "Test Accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Number of classes after filtering\n",
    "num_classes = len(selected_classes)  # This will be 10\n",
    "\n",
    "# Define the MobileNetV2 model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Reduces feature map dimensions\n",
    "        Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05)),  # Regularization\n",
    "        Dropout(0.5),  # Dropout for regularization\n",
    "        Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer dynamically set to 10\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (64, 64, 3)  # Image dimensions\n",
    "model = build_mobilenetv2(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for learning rate adjustment and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,  # Use a batch size suitable for your hardware\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune the base model\n",
    "base_model = model.layers[0]  # Extract base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Unfreeze the last 50 layers for fine-tuning\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mobilenet subeset accuracy 51% but overfits val acc is 37%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/b81lcpw16vngs5l3k2jfppy00000gn/T/ipykernel_6120/1643731749.py:9: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.2844 - loss: 1.7479 - val_accuracy: 0.3269 - val_loss: 1.3268\n",
      "Epoch 2/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3270 - loss: 1.3587 - val_accuracy: 0.3511 - val_loss: 1.2961\n",
      "Epoch 3/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3455 - loss: 1.3143 - val_accuracy: 0.3519 - val_loss: 1.2893\n",
      "Epoch 4/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3640 - loss: 1.2869 - val_accuracy: 0.3600 - val_loss: 1.2859\n",
      "Epoch 5/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3736 - loss: 1.2767 - val_accuracy: 0.3541 - val_loss: 1.2842\n",
      "Epoch 6/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3796 - loss: 1.2611 - val_accuracy: 0.3525 - val_loss: 1.2791\n",
      "Epoch 7/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3856 - loss: 1.2592 - val_accuracy: 0.3616 - val_loss: 1.2765\n",
      "Epoch 8/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4009 - loss: 1.2466 - val_accuracy: 0.3608 - val_loss: 1.2810\n",
      "Epoch 9/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3950 - loss: 1.2407 - val_accuracy: 0.3583 - val_loss: 1.2750\n",
      "Epoch 10/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4142 - loss: 1.2241 - val_accuracy: 0.3630 - val_loss: 1.2752\n",
      "Epoch 11/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4127 - loss: 1.2246 - val_accuracy: 0.3656 - val_loss: 1.2731\n",
      "Epoch 12/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4094 - loss: 1.2229 - val_accuracy: 0.3650 - val_loss: 1.2767\n",
      "Epoch 13/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4206 - loss: 1.2130 - val_accuracy: 0.3697 - val_loss: 1.2764\n",
      "Epoch 14/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4294 - loss: 1.2028 - val_accuracy: 0.3625 - val_loss: 1.2760\n",
      "Epoch 15/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4286 - loss: 1.1987 - val_accuracy: 0.3678 - val_loss: 1.2758\n",
      "Epoch 16/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4355 - loss: 1.1923 - val_accuracy: 0.3622 - val_loss: 1.2795\n",
      "Epoch 17/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4429 - loss: 1.1880 - val_accuracy: 0.3542 - val_loss: 1.2827\n",
      "Epoch 18/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4384 - loss: 1.1810 - val_accuracy: 0.3672 - val_loss: 1.2766\n",
      "Epoch 19/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4534 - loss: 1.1689 - val_accuracy: 0.3633 - val_loss: 1.2801\n",
      "Epoch 20/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4603 - loss: 1.1645 - val_accuracy: 0.3738 - val_loss: 1.2788\n",
      "Epoch 21/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4532 - loss: 1.1696 - val_accuracy: 0.3642 - val_loss: 1.2822\n",
      "Epoch 22/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4505 - loss: 1.1678 - val_accuracy: 0.3689 - val_loss: 1.2837\n",
      "Epoch 23/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4667 - loss: 1.1511 - val_accuracy: 0.3677 - val_loss: 1.2837\n",
      "Epoch 24/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4699 - loss: 1.1410 - val_accuracy: 0.3692 - val_loss: 1.2875\n",
      "Epoch 25/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4752 - loss: 1.1323 - val_accuracy: 0.3688 - val_loss: 1.2813\n",
      "Epoch 26/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4805 - loss: 1.1373 - val_accuracy: 0.3580 - val_loss: 1.2898\n",
      "Epoch 27/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4850 - loss: 1.1303 - val_accuracy: 0.3647 - val_loss: 1.2901\n",
      "Epoch 28/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4810 - loss: 1.1276 - val_accuracy: 0.3780 - val_loss: 1.2884\n",
      "Epoch 29/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4860 - loss: 1.1161 - val_accuracy: 0.3669 - val_loss: 1.2904\n",
      "Epoch 30/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.4862 - loss: 1.1142 - val_accuracy: 0.3664 - val_loss: 1.2917\n",
      "Epoch 31/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.5026 - loss: 1.0984 - val_accuracy: 0.3644 - val_loss: 1.2962\n",
      "Epoch 32/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 1.0930 - val_accuracy: 0.3673 - val_loss: 1.2973\n",
      "Epoch 33/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5090 - loss: 1.0884 - val_accuracy: 0.3698 - val_loss: 1.3011\n",
      "Epoch 34/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.5174 - loss: 1.0795 - val_accuracy: 0.3709 - val_loss: 1.3025\n",
      "Epoch 35/35\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.5111 - loss: 1.0777 - val_accuracy: 0.3591 - val_loss: 1.3193\n",
      "100/100 - 2s - 22ms/step - accuracy: 0.3713 - loss: 1.3192\n",
      "Test Accuracy: 0.37\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Pooling to reduce feature map dimensions\n",
    "        Dense(512, activation='relu'),  # Fully connected layer\n",
    "        Dropout(0.5),  # Dropout for regularization\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (64, 64, 3)  # Image size and RGB channels\n",
    "num_classes = len(selected_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_mobilenetv2(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=35,  # You can adjust based on performance\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy subeset mobilenet 37%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/b81lcpw16vngs5l3k2jfppy00000gn/T/ipykernel_6120/2151276436.py:8: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.2712 - loss: 2.2514 - val_accuracy: 0.3231 - val_loss: 1.3841\n",
      "Epoch 2/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.3140 - loss: 1.8318 - val_accuracy: 0.3373 - val_loss: 1.3478\n",
      "Epoch 3/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3184 - loss: 1.7268 - val_accuracy: 0.3475 - val_loss: 1.3198\n",
      "Epoch 4/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3189 - loss: 1.6261 - val_accuracy: 0.3503 - val_loss: 1.3034\n",
      "Epoch 5/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3201 - loss: 1.5701 - val_accuracy: 0.3552 - val_loss: 1.2982\n",
      "Epoch 6/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3336 - loss: 1.4992 - val_accuracy: 0.3583 - val_loss: 1.2891\n",
      "Epoch 7/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3349 - loss: 1.4642 - val_accuracy: 0.3523 - val_loss: 1.2944\n",
      "Epoch 8/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3388 - loss: 1.4354 - val_accuracy: 0.3580 - val_loss: 1.2916\n",
      "Epoch 9/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3414 - loss: 1.4024 - val_accuracy: 0.3586 - val_loss: 1.2842\n",
      "Epoch 10/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3393 - loss: 1.3861 - val_accuracy: 0.3584 - val_loss: 1.2824\n",
      "Epoch 11/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.3476 - loss: 1.3584 - val_accuracy: 0.3603 - val_loss: 1.2818\n",
      "Epoch 12/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3461 - loss: 1.3403 - val_accuracy: 0.3619 - val_loss: 1.2864\n",
      "Epoch 13/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3648 - loss: 1.3230 - val_accuracy: 0.3592 - val_loss: 1.2772\n",
      "Epoch 14/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3608 - loss: 1.3026 - val_accuracy: 0.3630 - val_loss: 1.2737\n",
      "Epoch 15/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3637 - loss: 1.3030 - val_accuracy: 0.3622 - val_loss: 1.2772\n",
      "Epoch 16/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3598 - loss: 1.2991 - val_accuracy: 0.3684 - val_loss: 1.2776\n",
      "Epoch 17/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3737 - loss: 1.2821 - val_accuracy: 0.3630 - val_loss: 1.2765\n",
      "Epoch 18/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3752 - loss: 1.2746 - val_accuracy: 0.3644 - val_loss: 1.2788\n",
      "Epoch 19/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 28ms/step - accuracy: 0.3776 - loss: 1.2755 - val_accuracy: 0.3609 - val_loss: 1.2757\n",
      "Epoch 20/20\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3753 - loss: 1.2736 - val_accuracy: 0.3691 - val_loss: 1.2744\n",
      "100/100 - 2s - 19ms/step - accuracy: 0.3675 - loss: 1.2806\n",
      "Test Accuracy: 0.37\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a deeper classification head for MobileNetV2\n",
    "def build_enhanced_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers initially\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Pooling to reduce dimensionality\n",
    "        Dense(1024, activation='relu'),  # Added depth with larger Dense layer\n",
    "        BatchNormalization(),  # Helps stabilize training\n",
    "        Dropout(0.5),  # Regularization\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Final output layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (64, 64, 3)  # Image size and RGB channels\n",
    "num_classes = len(selected_classes)\n",
    "\n",
    "# Build and compile the enhanced model\n",
    "model = build_enhanced_mobilenetv2(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,  # Adjust epochs based on performance\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model without hilbert with mobilnet accuracy 42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/b81lcpw16vngs5l3k2jfppy00000gn/T/ipykernel_6120/4049726855.py:9: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.2764 - loss: 1.7767 - val_accuracy: 0.3323 - val_loss: 1.3161\n",
      "Epoch 2/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.3428 - loss: 1.3454 - val_accuracy: 0.3477 - val_loss: 1.2984\n",
      "Epoch 3/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3507 - loss: 1.3135 - val_accuracy: 0.3505 - val_loss: 1.2901\n",
      "Epoch 4/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3563 - loss: 1.2941 - val_accuracy: 0.3494 - val_loss: 1.2855\n",
      "Epoch 5/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3700 - loss: 1.2757 - val_accuracy: 0.3542 - val_loss: 1.2865\n",
      "Epoch 6/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3854 - loss: 1.2631 - val_accuracy: 0.3577 - val_loss: 1.2776\n",
      "Epoch 7/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3806 - loss: 1.2582 - val_accuracy: 0.3623 - val_loss: 1.2769\n",
      "Epoch 8/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.3990 - loss: 1.2410 - val_accuracy: 0.3555 - val_loss: 1.2739\n",
      "Epoch 9/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.3990 - loss: 1.2389 - val_accuracy: 0.3539 - val_loss: 1.2784\n",
      "Epoch 10/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.4100 - loss: 1.2332 - val_accuracy: 0.3630 - val_loss: 1.2715\n",
      "Epoch 11/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.4058 - loss: 1.2284 - val_accuracy: 0.3666 - val_loss: 1.2699\n",
      "Epoch 12/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4256 - loss: 1.2172 - val_accuracy: 0.3634 - val_loss: 1.2746\n",
      "Epoch 13/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4215 - loss: 1.2119 - val_accuracy: 0.3622 - val_loss: 1.2743\n",
      "Epoch 14/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4209 - loss: 1.2128 - val_accuracy: 0.3670 - val_loss: 1.2713\n",
      "Epoch 15/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 0.4288 - loss: 1.1986 - val_accuracy: 0.3720 - val_loss: 1.2721\n",
      "100/100 - 2s - 21ms/step - accuracy: 0.3759 - loss: 1.2754\n",
      "Test Accuracy: 0.38\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the model\n",
    "def build_mobilenetv2(input_shape, num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),  # Pooling to reduce feature map dimensions\n",
    "        Dense(512, activation='relu'),  # Fully connected layer\n",
    "        Dropout(0.5),  # Dropout for regularization\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (64, 64, 3)  # Image size and RGB channels\n",
    "num_classes = len(selected_classes)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_mobilenetv2(input_shape, num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,  # You can adjust based on performance\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Hilberts Space Filling Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "\n",
    "\n",
    "def apply_hilbert_curve(image, n=2):\n",
    "    \"\"\"\n",
    "    Applies the Hilbert curve to reorder image pixels.\n",
    "    Args:\n",
    "        image: Input 2D grayscale or 3D RGB image (numpy array).\n",
    "        n: Number of dimensions for the Hilbert curve (default is 2D).\n",
    "    Returns:\n",
    "        Transformed image reordered using the Hilbert curve.\n",
    "    \"\"\"\n",
    "    if image.shape[0] != image.shape[1]:\n",
    "        raise ValueError(\"Image must be square for Hilbert curve transformation.\")\n",
    "\n",
    "    size = image.shape[0]  # Assuming square images\n",
    "    p = int(np.log2(size))  # Determine the order of the Hilbert curve\n",
    "\n",
    "    if 2**p != size:\n",
    "        raise ValueError(f\"Image size must be a power of 2. Current size: {size}\")\n",
    "\n",
    "    hilbert_curve = HilbertCurve(p, n)\n",
    "    coords = [hilbert_curve.point_from_distance(d) for d in range(2**(p * n))]\n",
    "\n",
    "    # Reorder image pixels based on Hilbert curve\n",
    "    flat_image = image.flatten()\n",
    "    reordered_image = np.zeros_like(flat_image)\n",
    "    for i, (x, y) in enumerate(coords):\n",
    "        reordered_image[i] = flat_image[x * size + y]\n",
    "\n",
    "    return reordered_image.reshape(image.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess only 8 classes for training\n",
    "def filter_classes(data, labels, classes, subset_size=8):\n",
    "    \"\"\"Filters data and labels to only include a subset of classes.\"\"\"\n",
    "    selected_classes = classes[:subset_size]\n",
    "    class_indices = [classes.index(cls) for cls in selected_classes]\n",
    "    filtered_data = []\n",
    "    filtered_labels = []\n",
    "    for img, lbl in zip(data, labels):\n",
    "        if lbl in class_indices:\n",
    "            filtered_data.append(img)\n",
    "            filtered_labels.append(lbl)\n",
    "    return np.array(filtered_data), np.array(filtered_labels), selected_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a subset of 9 classes (incremental training) with hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your existing code to load the dataset\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Filter the first 8 classes for training\n",
    "subset_size = 8\n",
    "filtered_data, filtered_labels, selected_classes = filter_classes(data, labels, classes, subset_size=subset_size)\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "filtered_data, filtered_labels = shuffle(filtered_data, filtered_labels, random_state=123)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(filtered_data, filtered_labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert numerical labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(selected_classes))\n",
    "y_val = to_categorical(y_val, num_classes=len(selected_classes))\n",
    "y_test = to_categorical(y_test, num_classes=len(selected_classes))\n",
    "\n",
    "\n",
    "# Apply Hilbert Curve to data\n",
    "X_train = np.array([apply_hilbert_curve(img) for img in X_train])\n",
    "X_val = np.array([apply_hilbert_curve(img) for img in X_val])\n",
    "X_test = np.array([apply_hilbert_curve(img) for img in X_test])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with hilbert as well subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish  \n",
    "\n",
    "def build_enhanced_alexnet(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # First Conv Block\n",
    "        Conv2D(96, (11, 11), strides=4, input_shape=input_shape, kernel_regularizer=l2(0.02)),\n",
    "        BatchNormalization(),\n",
    "        # Swish activation applied\n",
    "        AveragePooling2D((2, 2), strides=2),\n",
    "\n",
    "        # Second Conv Block\n",
    "        Conv2D(256, (5, 5), padding='same', kernel_regularizer=l2(0.02)),\n",
    "        BatchNormalization(),\n",
    "        AveragePooling2D((2, 2), strides=2),\n",
    "\n",
    "        # Third Conv Block\n",
    "        Conv2D(384, (3, 3), padding='same', kernel_regularizer=l2(0.02)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Fourth Conv Block\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        AveragePooling2D((2, 2), strides=2),\n",
    "\n",
    "        # Additional Conv Layer for feature enrichment\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling2D(),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        Dense(4096, kernel_regularizer=l2(0.02), activation=swish),  # Swish activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(2048, kernel_regularizer=l2(0.02), activation=swish),  # Swish activation\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 78ms/step - accuracy: 0.2641 - loss: 60.1118 - val_accuracy: 0.1253 - val_loss: 36.3542\n",
      "Epoch 2/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.2822 - loss: 24.8802 - val_accuracy: 0.1297 - val_loss: 24.5882\n",
      "Epoch 3/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2755 - loss: 9.7143 - val_accuracy: 0.1372 - val_loss: 19.3844\n",
      "Epoch 4/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2875 - loss: 4.6135 - val_accuracy: 0.2442 - val_loss: 7.6495\n",
      "Epoch 5/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 80ms/step - accuracy: 0.2893 - loss: 3.0477 - val_accuracy: 0.1214 - val_loss: 27.4130\n",
      "Epoch 6/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2824 - loss: 2.5058 - val_accuracy: 0.2125 - val_loss: 3.3555\n",
      "Epoch 7/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2854 - loss: 2.2686 - val_accuracy: 0.1203 - val_loss: 25.8767\n",
      "Epoch 8/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2914 - loss: 2.1769 - val_accuracy: 0.1214 - val_loss: 15.5666\n",
      "Epoch 9/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.2867 - loss: 2.0852 - val_accuracy: 0.1255 - val_loss: 34.1144\n",
      "Epoch 10/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2949 - loss: 2.0309 - val_accuracy: 0.1203 - val_loss: 52.9481\n",
      "Epoch 11/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 78ms/step - accuracy: 0.2868 - loss: 1.9710 - val_accuracy: 0.2134 - val_loss: 4.7536\n",
      "Epoch 12/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.2949 - loss: 1.9136 - val_accuracy: 0.1298 - val_loss: 72.0177\n",
      "Epoch 13/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - accuracy: 0.2981 - loss: 1.8661 - val_accuracy: 0.1327 - val_loss: 31.6529\n",
      "Epoch 14/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 80ms/step - accuracy: 0.2957 - loss: 1.8325 - val_accuracy: 0.1214 - val_loss: 21.5069\n",
      "Epoch 15/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 81ms/step - accuracy: 0.2920 - loss: 1.8174 - val_accuracy: 0.1214 - val_loss: 8.3147\n",
      "100/100 - 1s - 14ms/step - accuracy: 0.1209 - loss: 8.5354\n",
      "Test Accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "model = build_enhanced_alexnet(input_shape, len(selected_classes))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy 25 aber overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# AlexNet-inspired architecture with modifications\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(128, (11, 11), strides=4, activation=swish, input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D((3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))  # Regularization with dropout\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(256, (5, 5), activation=swish, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D((3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(384, (3, 3), activation=swish, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(384, (3, 3), activation=swish, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(512, (3, 3), activation=swish, padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D((3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # 1st Dense Layer\n",
    "    model.add(Dense(4096, activation=swish, kernel_regularizer=l2(0.01)))  # Added L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))  # Dropout to reduce overfitting\n",
    "\n",
    "    # 2nd Dense Layer\n",
    "    model.add(Dense(4096, activation=swish, kernel_regularizer=l2(0.01)))  # Added L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # 3rd Dense Layer\n",
    "    model.add(Dense(2048, activation=swish))  # No L2 regularization here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 167ms/step - accuracy: 0.2056 - loss: 53.1360 - val_accuracy: 0.1472 - val_loss: 42.9628 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 166ms/step - accuracy: 0.2440 - loss: 35.1721 - val_accuracy: 0.2603 - val_loss: 23.2538 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 165ms/step - accuracy: 0.2506 - loss: 20.7150 - val_accuracy: 0.2620 - val_loss: 12.6875 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 163ms/step - accuracy: 0.2474 - loss: 11.3557 - val_accuracy: 0.2338 - val_loss: 7.3159 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 159ms/step - accuracy: 0.2554 - loss: 6.2414 - val_accuracy: 0.2427 - val_loss: 4.1052 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 162ms/step - accuracy: 0.2517 - loss: 3.7598 - val_accuracy: 0.1117 - val_loss: 4.4493 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 159ms/step - accuracy: 0.2408 - loss: 2.6986 - val_accuracy: 0.2414 - val_loss: 2.1916 - learning_rate: 1.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 163ms/step - accuracy: 0.2527 - loss: 2.3096 - val_accuracy: 0.1295 - val_loss: 24.9149 - learning_rate: 1.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 158ms/step - accuracy: 0.2557 - loss: 2.1288 - val_accuracy: 0.2336 - val_loss: 3.5971 - learning_rate: 1.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 162ms/step - accuracy: 0.2505 - loss: 2.1020 - val_accuracy: 0.2617 - val_loss: 2.0676 - learning_rate: 1.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 161ms/step - accuracy: 0.2576 - loss: 2.1052 - val_accuracy: 0.2236 - val_loss: 1.8821 - learning_rate: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 160ms/step - accuracy: 0.2464 - loss: 2.0443 - val_accuracy: 0.2442 - val_loss: 3.3729 - learning_rate: 1.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 158ms/step - accuracy: 0.2524 - loss: 2.0164 - val_accuracy: 0.2594 - val_loss: 2.0781 - learning_rate: 1.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 159ms/step - accuracy: 0.2556 - loss: 2.0170 - val_accuracy: 0.1538 - val_loss: 2.4026 - learning_rate: 1.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 158ms/step - accuracy: 0.2540 - loss: 1.9217 - val_accuracy: 0.1564 - val_loss: 2.3400 - learning_rate: 5.0000e-05\n",
      "100/100 - 3s - 27ms/step - accuracy: 0.1475 - loss: 2.3549\n",
      "Test Accuracy: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "model = build_modified_alexnet(input_shape, len(selected_classes))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build AlexNet-inspired CNN pretty simple accuracy 33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alexnet(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2), strides=2),  \n",
    "        Conv2D(256, (5, 5), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2), strides=2),  \n",
    "        Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2), strides=2),  \n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 104ms/step - accuracy: 0.2107 - loss: 1.6719 - val_accuracy: 0.2683 - val_loss: 1.4084\n",
      "Epoch 2/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 103ms/step - accuracy: 0.2601 - loss: 1.4088 - val_accuracy: 0.2527 - val_loss: 1.3949\n",
      "Epoch 3/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 103ms/step - accuracy: 0.2749 - loss: 1.3954 - val_accuracy: 0.3042 - val_loss: 1.3665\n",
      "Epoch 4/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 105ms/step - accuracy: 0.3040 - loss: 1.3617 - val_accuracy: 0.3187 - val_loss: 1.3401\n",
      "Epoch 5/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 106ms/step - accuracy: 0.3106 - loss: 1.3503 - val_accuracy: 0.3159 - val_loss: 1.3399\n",
      "Epoch 6/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 106ms/step - accuracy: 0.3168 - loss: 1.3410 - val_accuracy: 0.3277 - val_loss: 1.3333\n",
      "Epoch 7/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 106ms/step - accuracy: 0.3187 - loss: 1.3353 - val_accuracy: 0.3161 - val_loss: 1.3392\n",
      "Epoch 8/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 108ms/step - accuracy: 0.3149 - loss: 1.3302 - val_accuracy: 0.3206 - val_loss: 1.3264\n",
      "Epoch 9/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 107ms/step - accuracy: 0.3259 - loss: 1.3233 - val_accuracy: 0.3184 - val_loss: 1.3368\n",
      "Epoch 10/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 107ms/step - accuracy: 0.3208 - loss: 1.3326 - val_accuracy: 0.3202 - val_loss: 1.3264\n",
      "Epoch 11/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 107ms/step - accuracy: 0.3266 - loss: 1.3164 - val_accuracy: 0.3280 - val_loss: 1.3272\n",
      "Epoch 12/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 111ms/step - accuracy: 0.3353 - loss: 1.3083 - val_accuracy: 0.3281 - val_loss: 1.3169\n",
      "Epoch 13/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 106ms/step - accuracy: 0.3363 - loss: 1.3108 - val_accuracy: 0.3183 - val_loss: 1.3263\n",
      "Epoch 14/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 112ms/step - accuracy: 0.3417 - loss: 1.3084 - val_accuracy: 0.3191 - val_loss: 1.3242\n",
      "Epoch 15/15\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 106ms/step - accuracy: 0.3413 - loss: 1.3068 - val_accuracy: 0.3223 - val_loss: 1.3219\n",
      "100/100 - 2s - 17ms/step - accuracy: 0.3356 - loss: 1.3194\n",
      "Test Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "model = build_alexnet(input_shape, len(selected_classes))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model 34% accuracy\n",
    "model.save('my_model8_subset_33.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50400, 64, 64, 3), (50400, 18)\n",
      "Validation data shape: (14400, 64, 64, 3), (14400, 18)\n",
      "Test data shape: (7200, 64, 64, 3), (7200, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Shuffle the dataset\n",
    "data, labels = shuffle(data, labels, random_state=123)\n",
    "\n",
    "# Train, validation, and test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert numerical labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(classes))\n",
    "y_val = to_categorical(y_val, num_classes=len(classes))\n",
    "y_test = to_categorical(y_test, num_classes=len(classes))\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select subset of data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (28001, 64, 64, 3)\n",
      "y_subset shape: (28001, 18)\n"
     ]
    }
   ],
   "source": [
    "# Select the first 9 classes numerically\n",
    "subset_class_indices = [i for i in range(10)]  # Classes 0, 1, 2...\n",
    "\n",
    "# Find indices of data points in these classes within the training data\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset_indices is not empty\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes. Check the logic.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer with L2 Regularization\n",
    "    model.add(Conv2D(128, (11, 11), strides=4, input_shape=input_shape, padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(SeparableConv2D(\n",
    "        512, (5, 5), padding='same', depthwise_regularizer=l2(0.01), pointwise_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # 3rd Convolutional Layer with L2 Regularization\n",
    "    model.add(SeparableConv2D(\n",
    "        768, (3, 3), padding='same', depthwise_regularizer=l2(0.01), pointwise_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.4))  # Added dropout\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(SeparableConv2D(768, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.4))  # Added dropout\n",
    "\n",
    "    # 5th Convolutional Layer with L2 Regularization\n",
    "    model.add(SeparableConv2D(\n",
    "        512, (3, 3), padding='same', depthwise_regularizer=l2(0.01), pointwise_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.5))  # Increased dropout\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))  # L2 regularization only here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, kernel_regularizer=l2(0.01)))  # L2 regularization only here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (28001, 64, 64, 3)\n",
      "y_subset shape: (28001, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Assuming the resized images are 64x64 with 3 color channels\n",
    "num_classes = len(classes)  # Total number of classes\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(10)]  # Select classes 0 to 9\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Callbacks for Early Stopping and Learning Rate Reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 68ms/step - accuracy: 0.1960 - loss: 18.8210 - val_accuracy: 0.0685 - val_loss: 53.7753 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 71ms/step - accuracy: 0.2024 - loss: 4.7850 - val_accuracy: 0.0549 - val_loss: 196.4526 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 71ms/step - accuracy: 0.2030 - loss: 4.4897 - val_accuracy: 0.1042 - val_loss: 11.5290 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 69ms/step - accuracy: 0.1977 - loss: 4.0129 - val_accuracy: 0.0831 - val_loss: 7.3711 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 69ms/step - accuracy: 0.2002 - loss: 3.9677 - val_accuracy: 0.0625 - val_loss: 10.7280 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 68ms/step - accuracy: 0.1987 - loss: 3.8972 - val_accuracy: 0.0558 - val_loss: 7.7072 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - accuracy: 0.2015 - loss: 3.8297 - val_accuracy: 0.0695 - val_loss: 8.2312 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 72ms/step - accuracy: 0.2022 - loss: 2.9039 - val_accuracy: 0.0563 - val_loss: 144.9415 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 69ms/step - accuracy: 0.2051 - loss: 2.5972 - val_accuracy: 0.1086 - val_loss: 24.1966 - learning_rate: 5.0000e-04\n",
      "Test Accuracy: 0.09\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    X_subset,\n",
    "    y_subset,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(128, (11, 11), strides=4, input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(SeparableConv2D(512, (5, 5), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(SeparableConv2D(768, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(SeparableConv2D(768, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.4))  # Increased dropout\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(SeparableConv2D(512, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.5))  # Increased dropout\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))  # Increased size\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))  # Increased size\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, kernel_regularizer=l2(0.01)))  # Added more layers\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Assuming the resized images are 64x64 with 3 color channels\n",
    "num_classes = len(classes)  # Total number of classes\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(10)]  # Select classes 0 to 9\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Callbacks for Early Stopping and Learning Rate Reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    X_subset,\n",
    "    y_subset,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (28001, 64, 64, 3)\n",
      "y_subset shape: (28001, 18)\n",
      "Epoch 1/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 38ms/step - accuracy: 0.2010 - loss: 11.1200 - val_accuracy: 0.0517 - val_loss: 57.2757 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 36ms/step - accuracy: 0.1960 - loss: 3.5387 - val_accuracy: 0.0515 - val_loss: 25.5853 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 37ms/step - accuracy: 0.1982 - loss: 3.3642 - val_accuracy: 0.0584 - val_loss: 10.6769 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 39ms/step - accuracy: 0.2038 - loss: 3.2414 - val_accuracy: 0.0491 - val_loss: 7.0207 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.2135 - loss: 3.1845 - val_accuracy: 0.1147 - val_loss: 6.7574 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.2110 - loss: 3.0236 - val_accuracy: 0.1124 - val_loss: 6.5164 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 38ms/step - accuracy: 0.2135 - loss: 2.8961 - val_accuracy: 0.0859 - val_loss: 7.8053 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.2157 - loss: 2.8697 - val_accuracy: 0.0960 - val_loss: 7.7451 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 36ms/step - accuracy: 0.2198 - loss: 2.8994 - val_accuracy: 0.0772 - val_loss: 8.1449 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 37ms/step - accuracy: 0.2233 - loss: 2.3505 - val_accuracy: 0.1174 - val_loss: 7.4649 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 37ms/step - accuracy: 0.2225 - loss: 2.2427 - val_accuracy: 0.1239 - val_loss: 7.8545 - learning_rate: 5.0000e-04\n",
      "Test Accuracy: 0.11\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(96, (11, 11), strides=4, input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))  # Added dropout to reduce overfitting\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (5, 5), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.3))  # Added dropout\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(SeparableConv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.3))  # Added dropout\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(SeparableConv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.3))  # Added dropout\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "    model.add(Dropout(0.4))  # Added dropout\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, kernel_regularizer=l2(0.01)))  # L2 regularization only here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, kernel_regularizer=l2(0.01)))  # L2 regularization only here\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Assuming the resized images are 64x64 with 3 color channels\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(10)]  # Classes 0, 1, 2 (numerical indices)\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Callbacks for Early Stopping and Learning Rate Reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    datagen.flow(X_subset, y_subset, batch_size=32),\n",
    "    epochs=30,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (8482, 64, 64, 3)\n",
      "y_subset shape: (8482, 18)\n",
      "Epoch 1/10\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 79ms/step - accuracy: 0.6685 - loss: 0.7060 - val_accuracy: 0.0551 - val_loss: 14.1518\n",
      "Epoch 2/10\n",
      "\u001b[1m 97/266\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.6829 - loss: 0.5431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_subset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_subset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Train the model on the subset\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     90\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.activations import swish\n",
    "import numpy as np\n",
    "\n",
    "# Define the AlexNet-based model with modifications\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(96, (11, 11), strides=4, input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (5, 5), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(SeparableConv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(SeparableConv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(4096))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Assuming the resized images are 64x64 with 3 color channels\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(3)]  # Classes 0, 1, 2 (numerical indices)\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    X_subset,\n",
    "    y_subset,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Activation, Conv1D, AveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import swish\n",
    "\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(96, (11, 11), strides=4, input_shape=input_shape, padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(Conv2D(256, (5, 5), padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(Conv2D(384, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(Conv2D(384, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (8453, 64, 64)\n",
      "y_subset shape: (8453, 18)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 64, 64), dtype=float32). Expected shape (None, 64, 64, 3), but input has incompatible shape (None, 64, 64)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 64, 64), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_subset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_subset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model on the subset\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     37\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/models/functional.py:273\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    271\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    272\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 64, 64), dtype=float32). Expected shape (None, 64, 64, 3), but input has incompatible shape (None, 64, 64)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 64, 64), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Keep 2D image format\n",
    "num_classes = len(classes)  # Number of classes\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(3)]  # Classes 0, 1, 2 (numerical indices)\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    X_subset,\n",
    "    y_subset,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_subset shape: (8453, 64, 64, 3)\n",
      "y_subset shape: (8453, 18)\n",
      "Epoch 1/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 87ms/step - accuracy: 0.6801 - loss: 17.9651 - val_accuracy: 0.0533 - val_loss: 5.4746\n",
      "Epoch 2/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 89ms/step - accuracy: 0.6986 - loss: 1.6870 - val_accuracy: 0.0533 - val_loss: 6.1013\n",
      "Epoch 3/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 88ms/step - accuracy: 0.7031 - loss: 2.0210 - val_accuracy: 0.0591 - val_loss: 34.9595\n",
      "Epoch 4/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 88ms/step - accuracy: 0.7123 - loss: 1.6760 - val_accuracy: 0.0618 - val_loss: 49.9560\n",
      "Epoch 5/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 91ms/step - accuracy: 0.7279 - loss: 2.0408 - val_accuracy: 0.0615 - val_loss: 61.5822\n",
      "Epoch 6/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 89ms/step - accuracy: 0.7168 - loss: 2.0844 - val_accuracy: 0.1081 - val_loss: 5.1303\n",
      "Epoch 7/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.7341 - loss: 1.8572 - val_accuracy: 0.0548 - val_loss: 79.8024\n",
      "Epoch 8/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.7370 - loss: 2.0812 - val_accuracy: 0.0548 - val_loss: 17.9617\n",
      "Epoch 9/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.7328 - loss: 1.8322 - val_accuracy: 0.0914 - val_loss: 30.6688\n",
      "Epoch 10/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.7265 - loss: 1.7821 - val_accuracy: 0.0548 - val_loss: 21.8397\n",
      "Epoch 11/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 89ms/step - accuracy: 0.7223 - loss: 1.9770 - val_accuracy: 0.0549 - val_loss: 148.4831\n",
      "Epoch 12/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 92ms/step - accuracy: 0.7231 - loss: 1.9246 - val_accuracy: 0.0549 - val_loss: 90.6379\n",
      "Epoch 13/20\n",
      "\u001b[1m124/265\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 68ms/step - accuracy: 0.7290 - loss: 3.2674"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_subset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_subset\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Train the model on the subset\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m    121\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv_2/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "\n",
    "# Define the AlexNet-based model with modifications\n",
    "def build_modified_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Convolutional Layer\n",
    "    model.add(Conv2D(96, (11, 11), strides=4, input_shape=input_shape, padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(SeparableConv2D(256, (5, 5), \n",
    "        padding='same', \n",
    "        depth_multiplier=1, \n",
    "        depthwise_regularizer=l2(0.01), \n",
    "        pointwise_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(SeparableConv2D(\n",
    "        384, \n",
    "        (3, 3), \n",
    "        padding='same', \n",
    "        depth_multiplier=1, \n",
    "        depthwise_regularizer=l2(0.01), \n",
    "        pointwise_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "\n",
    "    # 4th Convolutional Layer\n",
    "    model.add(SeparableConv2D(\n",
    "        384, \n",
    "        (3, 3), \n",
    "        padding='same', \n",
    "        depth_multiplier=1\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "\n",
    "    # 5th Convolutional Layer\n",
    "    model.add(SeparableConv2D(\n",
    "        256, \n",
    "        (3, 3), \n",
    "        padding='same', \n",
    "        depth_multiplier=1\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3), strides=2))\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(4096, kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(swish))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 3)  # Assuming the resized images are 64x64 with 3 color channels\n",
    "num_classes = len(classes)  # Number of classes in the dataset\n",
    "\n",
    "# Build the model\n",
    "model = build_modified_alexnet(input_shape, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Subset selection logic\n",
    "subset_class_indices = [i for i in range(3)]  # Classes 0, 1, 2 (numerical indices)\n",
    "subset_indices = [i for i, label in enumerate(np.argmax(y_train, axis=1)) if label in subset_class_indices]\n",
    "\n",
    "# Ensure subset indices are valid\n",
    "if len(subset_indices) == 0:\n",
    "    raise ValueError(\"No samples found for the selected subset classes.\")\n",
    "\n",
    "# Create subset data and labels\n",
    "X_subset = X_train[subset_indices]\n",
    "y_subset = y_train[subset_indices]\n",
    "\n",
    "# Debug: Print subset shapes\n",
    "print(f\"X_subset shape: {X_subset.shape}\")\n",
    "print(f\"y_subset shape: {y_subset.shape}\")\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    X_subset,\n",
    "    y_subset,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in data: 72000\n",
      "Subset indices: [] (showing first 10)\n",
      "Subset X shape: (0, 64, 64, 3)\n",
      "Subset y shape: (0, 18)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total samples in data: {data.shape[0]}\")\n",
    "print(f\"Subset indices: {subset_indices[:10]} (showing first 10)\")\n",
    "print(f\"Subset X shape: {X_subset.shape}\")\n",
    "print(f\"Subset y shape: {y_subset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Hilbert Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 64)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "hilbert_order = 5\n",
    "grid_size = 2 ** hilbert_order\n",
    "\n",
    "# Directory containing the dataset\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")\n",
    "\n",
    "# 1. Transform Images with Hilbert Curve\n",
    "def hilbert_transform(image, hilbert_order):\n",
    "    curve = HilbertCurve(hilbert_order, 2)\n",
    "    grid_size = 2 ** hilbert_order\n",
    "    indices = [curve.point_from_distance(i) for i in range(grid_size ** 2)]\n",
    "    transformed_image = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    flat_image = np.array(image).flatten()\n",
    "    for idx, value in zip(indices, flat_image):\n",
    "        transformed_image[idx[0], idx[1]] = value\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "def transform_dataset_with_hilbert(data, hilbert_order):\n",
    "    print(\"Applying Hilbert Curve Transformation...\")\n",
    "    transformed_data = []\n",
    "    for img in data:\n",
    "        # Ensure data is in uint8 and grayscale\n",
    "        if img.dtype != 'uint8':\n",
    "            img = (img * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8\n",
    "        img_gray = Image.fromarray(img).convert('L')  # Convert to grayscale\n",
    "        hilbert_image = hilbert_transform(img_gray, hilbert_order)\n",
    "        transformed_data.append(hilbert_image)\n",
    "    return np.array(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Applying Hilbert Curve Transformation...\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Data\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = Image.open(img_path).convert('RGB').resize(image_size)\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "data = data / 255.0  # Normalize pixel values\n",
    "data, labels = shuffle(data, labels, random_state=123)\n",
    "\n",
    "# 3. Apply Hilbert Transformation\n",
    "data = (data * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8\n",
    "hilbert_data = transform_dataset_with_hilbert(data, hilbert_order)\n",
    "\n",
    "# Reshape Hilbert-transformed data for CNN input\n",
    "hilbert_data = hilbert_data.reshape(-1, grid_size, grid_size, 1)\n",
    "\n",
    "# 4. Split Data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(hilbert_data, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(classes))\n",
    "y_val = to_categorical(y_val, num_classes=len(classes))\n",
    "y_test = to_categorical(y_test, num_classes=len(classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1023 - loss: 12.2590 - val_accuracy: 0.1328 - val_loss: 5.1922 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1233 - loss: 4.5318 - val_accuracy: 0.1334 - val_loss: 3.0109 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1253 - loss: 2.8797 - val_accuracy: 0.1335 - val_loss: 2.4348 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1345 - loss: 2.4252 - val_accuracy: 0.1428 - val_loss: 2.2699 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1374 - loss: 2.2929 - val_accuracy: 0.1471 - val_loss: 2.2173 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1406 - loss: 2.2439 - val_accuracy: 0.1373 - val_loss: 2.2235 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1416 - loss: 2.2208 - val_accuracy: 0.1422 - val_loss: 2.2010 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1483 - loss: 2.2033 - val_accuracy: 0.1403 - val_loss: 2.2220 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1488 - loss: 2.2012 - val_accuracy: 0.1397 - val_loss: 2.2377 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1492 - loss: 2.1921 - val_accuracy: 0.1488 - val_loss: 2.1819 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1514 - loss: 2.1900 - val_accuracy: 0.1549 - val_loss: 2.1787 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1532 - loss: 2.1842 - val_accuracy: 0.1516 - val_loss: 2.1762 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1512 - loss: 2.1836 - val_accuracy: 0.1201 - val_loss: 2.4075 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1575 - loss: 2.1807 - val_accuracy: 0.1485 - val_loss: 2.1873 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1543 - loss: 2.1745 - val_accuracy: 0.1491 - val_loss: 2.1823 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1559 - loss: 2.1731 - val_accuracy: 0.1407 - val_loss: 2.2699 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1613 - loss: 2.1696 - val_accuracy: 0.1305 - val_loss: 2.3419 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1615 - loss: 2.1571 - val_accuracy: 0.1491 - val_loss: 2.2134 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1658 - loss: 2.1475 - val_accuracy: 0.1560 - val_loss: 2.2017 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1684 - loss: 2.1421 - val_accuracy: 0.1517 - val_loss: 2.1840 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1700 - loss: 2.1342 - val_accuracy: 0.1518 - val_loss: 2.1793 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1707 - loss: 2.1363 - val_accuracy: 0.1577 - val_loss: 2.1905 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1790 - loss: 2.1193 - val_accuracy: 0.1590 - val_loss: 2.1824 - learning_rate: 2.5000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1778 - loss: 2.1102 - val_accuracy: 0.1601 - val_loss: 2.1962 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1864 - loss: 2.1022 - val_accuracy: 0.1513 - val_loss: 2.2832 - learning_rate: 2.5000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1886 - loss: 2.0925 - val_accuracy: 0.1547 - val_loss: 2.2188 - learning_rate: 2.5000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1891 - loss: 2.0857 - val_accuracy: 0.1600 - val_loss: 2.1985 - learning_rate: 2.5000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1977 - loss: 2.0704 - val_accuracy: 0.1596 - val_loss: 2.2175 - learning_rate: 1.2500e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1996 - loss: 2.0655 - val_accuracy: 0.1590 - val_loss: 2.2305 - learning_rate: 1.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1998 - loss: 2.0613 - val_accuracy: 0.1603 - val_loss: 2.2224 - learning_rate: 1.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2060 - loss: 2.0541 - val_accuracy: 0.1586 - val_loss: 2.2264 - learning_rate: 1.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2069 - loss: 2.0462 - val_accuracy: 0.1537 - val_loss: 2.2832 - learning_rate: 1.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2115 - loss: 2.0371 - val_accuracy: 0.1581 - val_loss: 2.2561 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2133 - loss: 2.0313 - val_accuracy: 0.1594 - val_loss: 2.2558 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2140 - loss: 2.0291 - val_accuracy: 0.1583 - val_loss: 2.2821 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2206 - loss: 2.0272 - val_accuracy: 0.1586 - val_loss: 2.2612 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2181 - loss: 2.0221 - val_accuracy: 0.1583 - val_loss: 2.3035 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2217 - loss: 2.0181 - val_accuracy: 0.1573 - val_loss: 2.3496 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2215 - loss: 2.0174 - val_accuracy: 0.1558 - val_loss: 2.3089 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2251 - loss: 2.0092 - val_accuracy: 0.1619 - val_loss: 2.2995 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2261 - loss: 2.0103 - val_accuracy: 0.1576 - val_loss: 2.3150 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.2269 - loss: 1.9983 - val_accuracy: 0.1544 - val_loss: 2.3293 - learning_rate: 1.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2314 - loss: 1.9960 - val_accuracy: 0.1596 - val_loss: 2.3278 - learning_rate: 1.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2294 - loss: 1.9900 - val_accuracy: 0.1570 - val_loss: 2.3599 - learning_rate: 1.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2350 - loss: 1.9838 - val_accuracy: 0.1530 - val_loss: 2.3304 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2384 - loss: 1.9828 - val_accuracy: 0.1561 - val_loss: 2.3618 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2408 - loss: 1.9797 - val_accuracy: 0.1572 - val_loss: 2.3653 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2375 - loss: 1.9782 - val_accuracy: 0.1569 - val_loss: 2.3545 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.2445 - loss: 1.9720 - val_accuracy: 0.1534 - val_loss: 2.3769 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.2469 - loss: 1.9659 - val_accuracy: 0.1550 - val_loss: 2.3358 - learning_rate: 1.0000e-05\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.1544 - loss: 2.3212\n",
      "Test Accuracy: 15.44%\n"
     ]
    }
   ],
   "source": [
    "# 5. CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        layers.Dropout(0.6),\n",
    "\n",
    "        # Output Layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5),\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acc 35% but overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1100 - loss: 5.8938 - val_accuracy: 0.1297 - val_loss: 4.3546 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1265 - loss: 4.1712 - val_accuracy: 0.1336 - val_loss: 3.3639 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1333 - loss: 3.2512 - val_accuracy: 0.1433 - val_loss: 2.7926 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1413 - loss: 2.7461 - val_accuracy: 0.1417 - val_loss: 2.5116 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1438 - loss: 2.4870 - val_accuracy: 0.1378 - val_loss: 2.3919 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1421 - loss: 2.3591 - val_accuracy: 0.1525 - val_loss: 2.2830 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1498 - loss: 2.2867 - val_accuracy: 0.1355 - val_loss: 2.3048 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1487 - loss: 2.2472 - val_accuracy: 0.1522 - val_loss: 2.2188 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1517 - loss: 2.2195 - val_accuracy: 0.1500 - val_loss: 2.2075 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1557 - loss: 2.2023 - val_accuracy: 0.1526 - val_loss: 2.2011 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1518 - loss: 2.1950 - val_accuracy: 0.1512 - val_loss: 2.1933 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1594 - loss: 2.1805 - val_accuracy: 0.1491 - val_loss: 2.1865 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1585 - loss: 2.1770 - val_accuracy: 0.1488 - val_loss: 2.1906 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1661 - loss: 2.1672 - val_accuracy: 0.1200 - val_loss: 2.3805 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1676 - loss: 2.1630 - val_accuracy: 0.1503 - val_loss: 2.2044 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1670 - loss: 2.1583 - val_accuracy: 0.1370 - val_loss: 2.2644 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1728 - loss: 2.1524 - val_accuracy: 0.1450 - val_loss: 2.2061 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1826 - loss: 2.1336 - val_accuracy: 0.1583 - val_loss: 2.1823 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1905 - loss: 2.1155 - val_accuracy: 0.1530 - val_loss: 2.2157 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1937 - loss: 2.1044 - val_accuracy: 0.1534 - val_loss: 2.2398 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2105 - loss: 2.0849 - val_accuracy: 0.1569 - val_loss: 2.2159 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2094 - loss: 2.0757 - val_accuracy: 0.1494 - val_loss: 2.2197 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2179 - loss: 2.0595 - val_accuracy: 0.1445 - val_loss: 2.6091 - learning_rate: 5.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2314 - loss: 2.0348 - val_accuracy: 0.1574 - val_loss: 2.2439 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2416 - loss: 2.0074 - val_accuracy: 0.1527 - val_loss: 2.2862 - learning_rate: 2.5000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2508 - loss: 1.9895 - val_accuracy: 0.1556 - val_loss: 2.3138 - learning_rate: 2.5000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2612 - loss: 1.9710 - val_accuracy: 0.1496 - val_loss: 2.3400 - learning_rate: 2.5000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2683 - loss: 1.9585 - val_accuracy: 0.1472 - val_loss: 2.3543 - learning_rate: 2.5000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2806 - loss: 1.9335 - val_accuracy: 0.1538 - val_loss: 2.3388 - learning_rate: 1.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2916 - loss: 1.9096 - val_accuracy: 0.1530 - val_loss: 2.3498 - learning_rate: 1.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2969 - loss: 1.8998 - val_accuracy: 0.1513 - val_loss: 2.3589 - learning_rate: 1.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3017 - loss: 1.8863 - val_accuracy: 0.1497 - val_loss: 2.3874 - learning_rate: 1.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3068 - loss: 1.8821 - val_accuracy: 0.1531 - val_loss: 2.3699 - learning_rate: 1.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.3197 - loss: 1.8589 - val_accuracy: 0.1504 - val_loss: 2.4139 - learning_rate: 6.2500e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.3166 - loss: 1.8567 - val_accuracy: 0.1504 - val_loss: 2.4225 - learning_rate: 6.2500e-06\n",
      "Epoch 36/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3282 - loss: 1.8483 - val_accuracy: 0.1505 - val_loss: 2.4373 - learning_rate: 6.2500e-06\n",
      "Epoch 37/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.3291 - loss: 1.8366 - val_accuracy: 0.1499 - val_loss: 2.4428 - learning_rate: 6.2500e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3354 - loss: 1.8262 - val_accuracy: 0.1500 - val_loss: 2.4502 - learning_rate: 6.2500e-06\n",
      "Epoch 39/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3396 - loss: 1.8226 - val_accuracy: 0.1497 - val_loss: 2.4551 - learning_rate: 3.1250e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3404 - loss: 1.8177 - val_accuracy: 0.1500 - val_loss: 2.4801 - learning_rate: 3.1250e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3432 - loss: 1.8121 - val_accuracy: 0.1508 - val_loss: 2.4782 - learning_rate: 3.1250e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3428 - loss: 1.8130 - val_accuracy: 0.1492 - val_loss: 2.4824 - learning_rate: 3.1250e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3460 - loss: 1.8057 - val_accuracy: 0.1504 - val_loss: 2.4705 - learning_rate: 3.1250e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3457 - loss: 1.8027 - val_accuracy: 0.1498 - val_loss: 2.4773 - learning_rate: 1.5625e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.3486 - loss: 1.7961 - val_accuracy: 0.1497 - val_loss: 2.4925 - learning_rate: 1.5625e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.3496 - loss: 1.7990 - val_accuracy: 0.1487 - val_loss: 2.4826 - learning_rate: 1.5625e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.3481 - loss: 1.7976 - val_accuracy: 0.1497 - val_loss: 2.4980 - learning_rate: 1.5625e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3498 - loss: 1.7987 - val_accuracy: 0.1489 - val_loss: 2.5000 - learning_rate: 1.5625e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.3524 - loss: 1.7936 - val_accuracy: 0.1491 - val_loss: 2.5075 - learning_rate: 1.0000e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.3571 - loss: 1.7880 - val_accuracy: 0.1503 - val_loss: 2.5016 - learning_rate: 1.0000e-06\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.1526 - loss: 2.4882\n",
      "Test Accuracy: 15.26%\n"
     ]
    }
   ],
   "source": [
    "# 5. CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model 19% accuracy\n",
    "cnn_model.save('my_model7_35.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy 17% with hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 40ms/step - accuracy: 0.1067 - loss: 7.1833 - val_accuracy: 0.1238 - val_loss: 4.3275 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1193 - loss: 3.8573 - val_accuracy: 0.1356 - val_loss: 2.9092 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 40ms/step - accuracy: 0.1269 - loss: 2.7672 - val_accuracy: 0.0997 - val_loss: 2.7183 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1355 - loss: 2.4058 - val_accuracy: 0.0975 - val_loss: 2.6556 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 40ms/step - accuracy: 0.1433 - loss: 2.2870 - val_accuracy: 0.1245 - val_loss: 2.2992 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 40ms/step - accuracy: 0.1450 - loss: 2.2388 - val_accuracy: 0.1322 - val_loss: 2.2413 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1473 - loss: 2.2179 - val_accuracy: 0.1131 - val_loss: 2.3338 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 40ms/step - accuracy: 0.1508 - loss: 2.2100 - val_accuracy: 0.1146 - val_loss: 2.5955 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 40ms/step - accuracy: 0.1471 - loss: 2.2017 - val_accuracy: 0.1377 - val_loss: 2.2170 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 40ms/step - accuracy: 0.1505 - loss: 2.1970 - val_accuracy: 0.1559 - val_loss: 2.1903 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1514 - loss: 2.1939 - val_accuracy: 0.1154 - val_loss: 2.3221 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 40ms/step - accuracy: 0.1502 - loss: 2.1895 - val_accuracy: 0.1281 - val_loss: 2.3251 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1516 - loss: 2.1893 - val_accuracy: 0.1375 - val_loss: 2.2676 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 40ms/step - accuracy: 0.1497 - loss: 2.1885 - val_accuracy: 0.1446 - val_loss: 2.2093 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1532 - loss: 2.1857 - val_accuracy: 0.1486 - val_loss: 2.2151 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1566 - loss: 2.1740 - val_accuracy: 0.1599 - val_loss: 2.1663 - learning_rate: 5.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1570 - loss: 2.1674 - val_accuracy: 0.1450 - val_loss: 2.1833 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1615 - loss: 2.1643 - val_accuracy: 0.1609 - val_loss: 2.1673 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1610 - loss: 2.1626 - val_accuracy: 0.1519 - val_loss: 2.1785 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1602 - loss: 2.1608 - val_accuracy: 0.1160 - val_loss: 2.3411 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1595 - loss: 2.1581 - val_accuracy: 0.1553 - val_loss: 2.1727 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1620 - loss: 2.1549 - val_accuracy: 0.1578 - val_loss: 2.1689 - learning_rate: 2.5000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1643 - loss: 2.1509 - val_accuracy: 0.1529 - val_loss: 2.1718 - learning_rate: 2.5000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1636 - loss: 2.1470 - val_accuracy: 0.1609 - val_loss: 2.1629 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1634 - loss: 2.1443 - val_accuracy: 0.1552 - val_loss: 2.1942 - learning_rate: 2.5000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1594 - loss: 2.1443 - val_accuracy: 0.1634 - val_loss: 2.1537 - learning_rate: 2.5000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1609 - loss: 2.1459 - val_accuracy: 0.1595 - val_loss: 2.1565 - learning_rate: 2.5000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1652 - loss: 2.1419 - val_accuracy: 0.1592 - val_loss: 2.1587 - learning_rate: 2.5000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1623 - loss: 2.1421 - val_accuracy: 0.1644 - val_loss: 2.1522 - learning_rate: 2.5000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1647 - loss: 2.1411 - val_accuracy: 0.1626 - val_loss: 2.1500 - learning_rate: 2.5000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1632 - loss: 2.1426 - val_accuracy: 0.1564 - val_loss: 2.1627 - learning_rate: 2.5000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1646 - loss: 2.1392 - val_accuracy: 0.1612 - val_loss: 2.1551 - learning_rate: 2.5000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1680 - loss: 2.1349 - val_accuracy: 0.1534 - val_loss: 2.1777 - learning_rate: 2.5000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1646 - loss: 2.1391 - val_accuracy: 0.1531 - val_loss: 2.1669 - learning_rate: 2.5000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1697 - loss: 2.1393 - val_accuracy: 0.1583 - val_loss: 2.1680 - learning_rate: 2.5000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1694 - loss: 2.1355 - val_accuracy: 0.1556 - val_loss: 2.1593 - learning_rate: 1.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1677 - loss: 2.1317 - val_accuracy: 0.1686 - val_loss: 2.1419 - learning_rate: 1.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1701 - loss: 2.1341 - val_accuracy: 0.1592 - val_loss: 2.1592 - learning_rate: 1.2500e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1724 - loss: 2.1271 - val_accuracy: 0.1654 - val_loss: 2.1512 - learning_rate: 1.2500e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1676 - loss: 2.1304 - val_accuracy: 0.1615 - val_loss: 2.1501 - learning_rate: 1.2500e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1739 - loss: 2.1247 - val_accuracy: 0.1625 - val_loss: 2.1496 - learning_rate: 1.2500e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1717 - loss: 2.1311 - val_accuracy: 0.1580 - val_loss: 2.1536 - learning_rate: 1.2500e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1741 - loss: 2.1244 - val_accuracy: 0.1642 - val_loss: 2.1462 - learning_rate: 6.2500e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1704 - loss: 2.1229 - val_accuracy: 0.1635 - val_loss: 2.1484 - learning_rate: 6.2500e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.1765 - loss: 2.1221 - val_accuracy: 0.1637 - val_loss: 2.1456 - learning_rate: 6.2500e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1709 - loss: 2.1221 - val_accuracy: 0.1649 - val_loss: 2.1418 - learning_rate: 6.2500e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1718 - loss: 2.1237 - val_accuracy: 0.1612 - val_loss: 2.1482 - learning_rate: 6.2500e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1718 - loss: 2.1204 - val_accuracy: 0.1643 - val_loss: 2.1515 - learning_rate: 6.2500e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1739 - loss: 2.1198 - val_accuracy: 0.1608 - val_loss: 2.1482 - learning_rate: 6.2500e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 41ms/step - accuracy: 0.1757 - loss: 2.1213 - val_accuracy: 0.1670 - val_loss: 2.1403 - learning_rate: 6.2500e-06\n",
      "225/225 - 2s - 10ms/step - accuracy: 0.1674 - loss: 2.1328\n",
      "Test Accuracy: 16.74%\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        # Global Pooling instead of Flatten\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model 19% accuracy\n",
    "cnn_model.save('my_model8_17.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 17ms/step - accuracy: 0.1055 - loss: 2.6677 - val_accuracy: 0.1233 - val_loss: 2.3539 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 16ms/step - accuracy: 0.1176 - loss: 2.4010 - val_accuracy: 0.1256 - val_loss: 2.3329 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1265 - loss: 2.3522 - val_accuracy: 0.1297 - val_loss: 2.3005 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1263 - loss: 2.3200 - val_accuracy: 0.1249 - val_loss: 2.2853 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1257 - loss: 2.2928 - val_accuracy: 0.1388 - val_loss: 2.2585 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1313 - loss: 2.2686 - val_accuracy: 0.1416 - val_loss: 2.2291 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1310 - loss: 2.2528 - val_accuracy: 0.1397 - val_loss: 2.2245 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1381 - loss: 2.2358 - val_accuracy: 0.1449 - val_loss: 2.2114 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1379 - loss: 2.2250 - val_accuracy: 0.1428 - val_loss: 2.2152 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1408 - loss: 2.2150 - val_accuracy: 0.1519 - val_loss: 2.1970 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1429 - loss: 2.2033 - val_accuracy: 0.1475 - val_loss: 2.1876 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1392 - loss: 2.2009 - val_accuracy: 0.1494 - val_loss: 2.1784 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m 854/1575\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.1488 - loss: 2.1919"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 58\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     53\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m),\n\u001b[1;32m     54\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m ]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Evaluate Model\u001b[39;00m\n\u001b[1;32m     67\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m hybrid_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, LSTM, TimeDistributed, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_hybrid_model(input_shape, num_classes):\n",
    "    model_input = Input(shape=input_shape)\n",
    "\n",
    "    # CNN Layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Flatten CNN output\n",
    "    x = Flatten()(x)\n",
    "    flattened_size = x.shape[-1]  # Get the flattened size dynamically\n",
    "\n",
    "    # Reshape for LSTM\n",
    "    reshape_target = (flattened_size // 32, 32)  # Ensure the total size matches\n",
    "    x = Reshape(reshape_target)(x)\n",
    "\n",
    "    # LSTM Layers\n",
    "    x = LSTM(128, activation='tanh', return_sequences=False)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Final Dense Layer for Classification\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=model_input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Compile Model\n",
    "hybrid_model = create_hybrid_model(input_shape=(grid_size, grid_size, 1), num_classes=len(classes))\n",
    "hybrid_model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = hybrid_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = hybrid_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.1012 - loss: 3.0761 - val_accuracy: 0.1194 - val_loss: 2.5480 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1192 - loss: 2.6105 - val_accuracy: 0.1236 - val_loss: 2.4722 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1187 - loss: 2.5233 - val_accuracy: 0.1277 - val_loss: 2.4353 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1229 - loss: 2.4654 - val_accuracy: 0.1310 - val_loss: 2.4006 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1293 - loss: 2.4202 - val_accuracy: 0.1212 - val_loss: 2.3984 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1324 - loss: 2.3755 - val_accuracy: 0.1199 - val_loss: 2.5314 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1347 - loss: 2.3436 - val_accuracy: 0.1309 - val_loss: 2.3128 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1341 - loss: 2.3160 - val_accuracy: 0.1376 - val_loss: 2.2883 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1351 - loss: 2.2895 - val_accuracy: 0.1367 - val_loss: 2.2726 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1369 - loss: 2.2725 - val_accuracy: 0.1367 - val_loss: 2.2562 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1404 - loss: 2.2515 - val_accuracy: 0.1435 - val_loss: 2.2343 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1414 - loss: 2.2391 - val_accuracy: 0.1476 - val_loss: 2.2240 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1435 - loss: 2.2238 - val_accuracy: 0.1308 - val_loss: 2.2325 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1446 - loss: 2.2156 - val_accuracy: 0.1533 - val_loss: 2.2025 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1438 - loss: 2.2082 - val_accuracy: 0.1529 - val_loss: 2.1910 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1495 - loss: 2.1973 - val_accuracy: 0.1459 - val_loss: 2.1966 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.1494 - loss: 2.1929 - val_accuracy: 0.1435 - val_loss: 2.2007 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - accuracy: 0.1505 - loss: 2.1875 - val_accuracy: 0.1560 - val_loss: 2.1803 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.1491 - loss: 2.1806 - val_accuracy: 0.1518 - val_loss: 2.1825 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.1490 - loss: 2.1776 - val_accuracy: 0.1433 - val_loss: 2.2104 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1521 - loss: 2.1748 - val_accuracy: 0.1471 - val_loss: 2.2362 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.1489 - loss: 2.1733 - val_accuracy: 0.1267 - val_loss: 2.3463 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - accuracy: 0.1542 - loss: 2.1685 - val_accuracy: 0.1567 - val_loss: 2.1660 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.1507 - loss: 2.1673 - val_accuracy: 0.1501 - val_loss: 2.1766 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.1550 - loss: 2.1654 - val_accuracy: 0.1585 - val_loss: 2.1669 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m 705/1575\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.1516 - loss: 2.1662"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     38\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m),\n\u001b[1;32m     39\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m ]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Evaluate Model\u001b[39;00m\n\u001b[1;32m     52\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5. Improved CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),  \n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test acc 15%, train 27% with Hilbert Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1081 - loss: 2.8622 - val_accuracy: 0.1198 - val_loss: 2.2661 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1253 - loss: 2.3208 - val_accuracy: 0.1410 - val_loss: 2.1927 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1294 - loss: 2.2543 - val_accuracy: 0.1329 - val_loss: 2.1841 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1335 - loss: 2.2183 - val_accuracy: 0.1489 - val_loss: 2.1620 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1423 - loss: 2.1905 - val_accuracy: 0.1468 - val_loss: 2.1635 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1488 - loss: 2.1734 - val_accuracy: 0.1471 - val_loss: 2.1588 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1519 - loss: 2.1567 - val_accuracy: 0.1358 - val_loss: 2.1760 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1584 - loss: 2.1456 - val_accuracy: 0.1538 - val_loss: 2.1446 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1609 - loss: 2.1376 - val_accuracy: 0.1513 - val_loss: 2.1542 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1657 - loss: 2.1306 - val_accuracy: 0.1510 - val_loss: 2.1467 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1659 - loss: 2.1240 - val_accuracy: 0.1525 - val_loss: 2.1451 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1732 - loss: 2.1155 - val_accuracy: 0.1544 - val_loss: 2.1538 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1772 - loss: 2.1088 - val_accuracy: 0.1538 - val_loss: 2.1441 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1801 - loss: 2.0990 - val_accuracy: 0.1544 - val_loss: 2.1615 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1863 - loss: 2.0911 - val_accuracy: 0.1490 - val_loss: 2.1873 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1890 - loss: 2.0792 - val_accuracy: 0.1493 - val_loss: 2.1659 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1954 - loss: 2.0724 - val_accuracy: 0.1444 - val_loss: 2.1943 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2058 - loss: 2.0528 - val_accuracy: 0.1502 - val_loss: 2.2139 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2118 - loss: 2.0352 - val_accuracy: 0.1526 - val_loss: 2.1708 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2175 - loss: 2.0202 - val_accuracy: 0.1542 - val_loss: 2.1709 - learning_rate: 5.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2269 - loss: 2.0088 - val_accuracy: 0.1504 - val_loss: 2.2057 - learning_rate: 5.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2310 - loss: 1.9977 - val_accuracy: 0.1542 - val_loss: 2.1850 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2339 - loss: 1.9886 - val_accuracy: 0.1478 - val_loss: 2.1937 - learning_rate: 5.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2397 - loss: 1.9749 - val_accuracy: 0.1533 - val_loss: 2.1953 - learning_rate: 2.5000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2432 - loss: 1.9641 - val_accuracy: 0.1526 - val_loss: 2.1980 - learning_rate: 2.5000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2500 - loss: 1.9536 - val_accuracy: 0.1540 - val_loss: 2.2022 - learning_rate: 2.5000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2528 - loss: 1.9509 - val_accuracy: 0.1557 - val_loss: 2.2116 - learning_rate: 2.5000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2547 - loss: 1.9465 - val_accuracy: 0.1542 - val_loss: 2.2174 - learning_rate: 2.5000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2584 - loss: 1.9335 - val_accuracy: 0.1524 - val_loss: 2.2227 - learning_rate: 1.2500e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2641 - loss: 1.9276 - val_accuracy: 0.1542 - val_loss: 2.2191 - learning_rate: 1.2500e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2629 - loss: 1.9225 - val_accuracy: 0.1515 - val_loss: 2.2271 - learning_rate: 1.2500e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2641 - loss: 1.9192 - val_accuracy: 0.1535 - val_loss: 2.2276 - learning_rate: 1.2500e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2646 - loss: 1.9178 - val_accuracy: 0.1503 - val_loss: 2.2392 - learning_rate: 1.2500e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2638 - loss: 1.9154 - val_accuracy: 0.1518 - val_loss: 2.2268 - learning_rate: 6.2500e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2682 - loss: 1.9110 - val_accuracy: 0.1526 - val_loss: 2.2304 - learning_rate: 6.2500e-06\n",
      "Epoch 36/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2715 - loss: 1.9036 - val_accuracy: 0.1524 - val_loss: 2.2320 - learning_rate: 6.2500e-06\n",
      "Epoch 37/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2694 - loss: 1.9097 - val_accuracy: 0.1533 - val_loss: 2.2326 - learning_rate: 6.2500e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2697 - loss: 1.9096 - val_accuracy: 0.1515 - val_loss: 2.2373 - learning_rate: 6.2500e-06\n",
      "Epoch 39/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2726 - loss: 1.9009 - val_accuracy: 0.1512 - val_loss: 2.2344 - learning_rate: 3.1250e-06\n",
      "Epoch 40/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2765 - loss: 1.9010 - val_accuracy: 0.1509 - val_loss: 2.2379 - learning_rate: 3.1250e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2716 - loss: 1.9022 - val_accuracy: 0.1503 - val_loss: 2.2402 - learning_rate: 3.1250e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2702 - loss: 1.9083 - val_accuracy: 0.1503 - val_loss: 2.2358 - learning_rate: 3.1250e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2736 - loss: 1.8990 - val_accuracy: 0.1499 - val_loss: 2.2362 - learning_rate: 3.1250e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2727 - loss: 1.8966 - val_accuracy: 0.1490 - val_loss: 2.2375 - learning_rate: 1.5625e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2752 - loss: 1.8943 - val_accuracy: 0.1504 - val_loss: 2.2377 - learning_rate: 1.5625e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.2758 - loss: 1.8972 - val_accuracy: 0.1496 - val_loss: 2.2393 - learning_rate: 1.5625e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2723 - loss: 1.8963 - val_accuracy: 0.1504 - val_loss: 2.2381 - learning_rate: 1.5625e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2755 - loss: 1.8994 - val_accuracy: 0.1494 - val_loss: 2.2387 - learning_rate: 1.5625e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.2738 - loss: 1.8968 - val_accuracy: 0.1490 - val_loss: 2.2407 - learning_rate: 1.0000e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2760 - loss: 1.8975 - val_accuracy: 0.1494 - val_loss: 2.2395 - learning_rate: 1.0000e-06\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.1538 - loss: 2.2387\n",
      "Test Accuracy: 15.38%\n"
     ]
    }
   ],
   "source": [
    "# 5. CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Vorhergesagte Klasse: 17\n",
      "Wahre Klasse: 1\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.1538 - loss: 2.2387\n",
      "Test Accuracy: 15.38%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model 19% accuracy\n",
    "cnn_model.save('my_model5_27.keras')\n",
    "\n",
    "# Load des Modells\n",
    "model = load_model('my_model5_27.keras')\n",
    "\n",
    "predictions = cnn_model.predict(X_test)\n",
    "print(\"Vorhergesagte Klasse:\", np.argmax(predictions[0]))\n",
    "print(\"Wahre Klasse:\", np.argmax(y_test[0]))\n",
    "\n",
    "# Testen des Modells\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Testgenauigkeit in Prozent ausgeben\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy 15% val and 23% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Applying Hilbert Curve Transformation...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1114 - loss: 2.7660 - val_accuracy: 0.1310 - val_loss: 2.2660 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1271 - loss: 2.3229 - val_accuracy: 0.1401 - val_loss: 2.1977 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1350 - loss: 2.2474 - val_accuracy: 0.1435 - val_loss: 2.1802 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1382 - loss: 2.2097 - val_accuracy: 0.1478 - val_loss: 2.1581 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1449 - loss: 2.1850 - val_accuracy: 0.1460 - val_loss: 2.1575 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1507 - loss: 2.1670 - val_accuracy: 0.1452 - val_loss: 2.1569 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1575 - loss: 2.1517 - val_accuracy: 0.1407 - val_loss: 2.1757 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1574 - loss: 2.1427 - val_accuracy: 0.1477 - val_loss: 2.1534 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1659 - loss: 2.1280 - val_accuracy: 0.1424 - val_loss: 2.1545 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.1730 - loss: 2.1229 - val_accuracy: 0.1579 - val_loss: 2.1441 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1749 - loss: 2.1105 - val_accuracy: 0.1503 - val_loss: 2.1476 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.1893 - loss: 2.0954 - val_accuracy: 0.1350 - val_loss: 2.1892 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1915 - loss: 2.0884 - val_accuracy: 0.1475 - val_loss: 2.1730 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.1982 - loss: 2.0785 - val_accuracy: 0.1358 - val_loss: 2.2101 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.2023 - loss: 2.0624 - val_accuracy: 0.1488 - val_loss: 2.1599 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2118 - loss: 2.0435 - val_accuracy: 0.1544 - val_loss: 2.1677 - learning_rate: 5.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.2181 - loss: 2.0293 - val_accuracy: 0.1551 - val_loss: 2.1674 - learning_rate: 5.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2276 - loss: 2.0142 - val_accuracy: 0.1551 - val_loss: 2.1726 - learning_rate: 5.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2360 - loss: 1.9969 - val_accuracy: 0.1581 - val_loss: 2.1824 - learning_rate: 5.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.2375 - loss: 1.9875 - val_accuracy: 0.1499 - val_loss: 2.1953 - learning_rate: 5.0000e-05\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.1582 - loss: 2.1385\n",
      "Test Accuracy: 15.82%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 64)\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "hilbert_order = 5\n",
    "grid_size = 2 ** hilbert_order\n",
    "\n",
    "# Directory containing the dataset\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")\n",
    "\n",
    "# 1. Transform Images with Hilbert Curve\n",
    "def hilbert_transform(image, hilbert_order):\n",
    "    curve = HilbertCurve(hilbert_order, 2)\n",
    "    grid_size = 2 ** hilbert_order\n",
    "    indices = [curve.point_from_distance(i) for i in range(grid_size ** 2)]\n",
    "    transformed_image = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    flat_image = np.array(image).flatten()\n",
    "    for idx, value in zip(indices, flat_image):\n",
    "        transformed_image[idx[0], idx[1]] = value\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "def transform_dataset_with_hilbert(data, hilbert_order):\n",
    "    print(\"Applying Hilbert Curve Transformation...\")\n",
    "    transformed_data = []\n",
    "    for img in data:\n",
    "        # Ensure data is in uint8 and grayscale\n",
    "        if img.dtype != 'uint8':\n",
    "            img = (img * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8\n",
    "        img_gray = Image.fromarray(img).convert('L')  # Convert to grayscale\n",
    "        hilbert_image = hilbert_transform(img_gray, hilbert_order)\n",
    "        transformed_data.append(hilbert_image)\n",
    "    return np.array(transformed_data)\n",
    "\n",
    "# 2. Load Data\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = Image.open(img_path).convert('RGB').resize(image_size)\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "data = data / 255.0  # Normalize pixel values\n",
    "data, labels = shuffle(data, labels, random_state=123)\n",
    "\n",
    "# 3. Apply Hilbert Transformation\n",
    "data = (data * 255).astype('uint8')  # Scale to [0, 255] and convert to uint8\n",
    "hilbert_data = transform_dataset_with_hilbert(data, hilbert_order)\n",
    "\n",
    "# Reshape Hilbert-transformed data for CNN input\n",
    "hilbert_data = hilbert_data.reshape(-1, grid_size, grid_size, 1)\n",
    "\n",
    "# 4. Split Data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(hilbert_data, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=123)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(classes))\n",
    "y_val = to_categorical(y_val, num_classes=len(classes))\n",
    "y_test = to_categorical(y_test, num_classes=len(classes))\n",
    "\n",
    "# 5. CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes=len(classes))\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    #tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (72000, 64, 64, 3)\n",
      "Data dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Data dtype:\", data.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space-Filling Curve Transformation, 10% weird images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hilbertcurve.hilbertcurve import HilbertCurve\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Parameters\n",
    "image_size = 64  # Original image size\n",
    "hilbert_order = 5  # Order of the Hilbert curve\n",
    "grid_size = 2 ** hilbert_order  # Grid size for Hilbert curve\n",
    "num_classes = 18  # Number of classes\n",
    "batch_size = 32\n",
    "epochs = 50  # Increased epochs for better training\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Space-Filling Curve Transformation\n",
    "def transform_with_hilbert(image, hilbert_order):\n",
    "    curve = HilbertCurve(hilbert_order, 2)  # Create 2D Hilbert curve\n",
    "    grid_size = 2 ** hilbert_order\n",
    "    indices = [curve.point_from_distance(i) for i in range(grid_size ** 2)]\n",
    "    transformed_image = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    flat_image = image.flatten()\n",
    "    for idx, value in zip(indices, flat_image):\n",
    "        transformed_image[idx[0], idx[1]] = value\n",
    "\n",
    "    transformed_image /= np.max(transformed_image)  # Normalize\n",
    "    return transformed_image\n",
    "\n",
    "# Transform Dataset\n",
    "def transform_dataset(images, hilbert_order):\n",
    "    print(f\"Transforming dataset using Hilbert curve of order {hilbert_order}...\")\n",
    "    transformed_images = []\n",
    "    for image in tqdm(images):\n",
    "        transformed_image = transform_with_hilbert(image, hilbert_order)\n",
    "        transformed_images.append(transformed_image)\n",
    "    return np.array(transformed_images)\n",
    "\n",
    "# Load Dataset\n",
    "def load_dataset(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(image_size, image_size))\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize images\n",
    "                data.append(img)\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Dataset and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset using Hilbert curve of order 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72000/72000 [02:52<00:00, 416.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")\n",
    "images, labels, classes = load_dataset(main_dir, image_size)\n",
    "\n",
    "# Transform the dataset\n",
    "transformed_images = transform_dataset(images, hilbert_order)\n",
    "\n",
    "# Reshape to match CNN input (add channel dimension for grayscale)\n",
    "transformed_images = transformed_images.reshape(-1, grid_size, grid_size, 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(transformed_images, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', classes=np.unique(np.argmax(labels, axis=1)), y=np.argmax(labels, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0897 - loss: 6.0163 - val_accuracy: 0.1035 - val_loss: 4.5516 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.1016 - loss: 4.3292 - val_accuracy: 0.0991 - val_loss: 3.5759 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 17ms/step - accuracy: 0.1035 - loss: 3.4621 - val_accuracy: 0.1029 - val_loss: 3.0143 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.1050 - loss: 2.9607 - val_accuracy: 0.1051 - val_loss: 2.7235 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step - accuracy: 0.1074 - loss: 2.6973 - val_accuracy: 0.1056 - val_loss: 2.5825 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 21ms/step - accuracy: 0.1047 - loss: 2.5641 - val_accuracy: 0.1011 - val_loss: 2.5224 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 20ms/step - accuracy: 0.1081 - loss: 2.4991 - val_accuracy: 0.1070 - val_loss: 2.4900 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 19ms/step - accuracy: 0.1072 - loss: 2.4624 - val_accuracy: 0.1025 - val_loss: 2.4472 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.1033 - loss: 2.4445 - val_accuracy: 0.0821 - val_loss: 2.5728 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.1041 - loss: 2.4382 - val_accuracy: 0.1049 - val_loss: 2.4425 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m 118/1575\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 19ms/step - accuracy: 0.1050 - loss: 2.4378"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     39\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m),\n\u001b[1;32m     40\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m ]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the Model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes)\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Cast data to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Train the Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of samples to check\n",
    "num_samples = 10\n",
    "\n",
    "# Randomly select samples to check\n",
    "indices = np.random.choice(len(X_train), num_samples, replace=False)\n",
    "\n",
    "for i in indices:\n",
    "    plt.imshow(X_train[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Label: {np.argmax(y_train[i])}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space filling curve 10% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset using Hilbert curve of order 5...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.0681 - loss: 2.8443 - val_accuracy: 0.0972 - val_loss: 2.5011 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1001 - loss: 2.5309 - val_accuracy: 0.1007 - val_loss: 2.4551 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1011 - loss: 2.4824 - val_accuracy: 0.1006 - val_loss: 2.4362 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0999 - loss: 2.4621 - val_accuracy: 0.1047 - val_loss: 2.4254 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1036 - loss: 2.4475 - val_accuracy: 0.1031 - val_loss: 2.4195 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1034 - loss: 2.4360 - val_accuracy: 0.1030 - val_loss: 2.4139 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1011 - loss: 2.4309 - val_accuracy: 0.1069 - val_loss: 2.4110 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1058 - loss: 2.4212 - val_accuracy: 0.1063 - val_loss: 2.4079 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1047 - loss: 2.4201 - val_accuracy: 0.1043 - val_loss: 2.4080 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1070 - loss: 2.4103 - val_accuracy: 0.1070 - val_loss: 2.4043 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1102 - loss: 2.4060 - val_accuracy: 0.1065 - val_loss: 2.4033 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1027 - loss: 2.4100 - val_accuracy: 0.1052 - val_loss: 2.4004 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1069 - loss: 2.4021 - val_accuracy: 0.1040 - val_loss: 2.3990 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1045 - loss: 2.4039 - val_accuracy: 0.1056 - val_loss: 2.3993 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1073 - loss: 2.4006 - val_accuracy: 0.1085 - val_loss: 2.3974 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1072 - loss: 2.4043 - val_accuracy: 0.1050 - val_loss: 2.3952 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1105 - loss: 2.3960 - val_accuracy: 0.1075 - val_loss: 2.3948 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.1081 - loss: 2.3997 - val_accuracy: 0.1096 - val_loss: 2.3940 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.1102 - loss: 2.3937 - val_accuracy: 0.1037 - val_loss: 2.3930 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1106 - loss: 2.4012 - val_accuracy: 0.1107 - val_loss: 2.3916 - learning_rate: 1.0000e-04\n",
      "338/338 - 1s - 2ms/step - accuracy: 0.1096 - loss: 2.3901\n",
      "Test Accuracy: 10.96%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "image_size = 64  # Size of your original images\n",
    "hilbert_order = 5  # Order of the Hilbert curve\n",
    "grid_size = 2 ** hilbert_order  # The size of the transformed image grid\n",
    "num_classes = 18  # Number of classes\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Hilbert Curve Implementation\n",
    "def hilbert_curve(order, index):\n",
    "    \"\"\"Compute the 2D coordinates of a point on a Hilbert curve of a given order.\"\"\"\n",
    "    x, y = 0, 0\n",
    "    for s in range(order):\n",
    "        mask = 1 << s\n",
    "        rx = 1 & (index // 2)\n",
    "        ry = 1 & (index ^ rx)\n",
    "        if ry == 0:\n",
    "            if rx == 1:\n",
    "                x, y = mask - 1 - x, mask - 1 - y\n",
    "            x, y = y, x\n",
    "        x += mask * rx\n",
    "        y += mask * ry\n",
    "        index //= 4\n",
    "    return x, y\n",
    "\n",
    "# Space-Filling Curve Transformation\n",
    "def transform_with_hilbert(image, hilbert_order):\n",
    "    \"\"\"Transform a 1D image into a 2D grid using a Hilbert curve.\"\"\"\n",
    "    grid_size = 2 ** hilbert_order\n",
    "    indices = [hilbert_curve(hilbert_order, i) for i in range(grid_size ** 2)]\n",
    "    transformed_image = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    flat_image = image.flatten()\n",
    "    for idx, value in zip(indices, flat_image):\n",
    "        transformed_image[idx[0], idx[1]] = value\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "# Transform Dataset\n",
    "def transform_dataset(images, hilbert_order):\n",
    "    print(f\"Transforming dataset using Hilbert curve of order {hilbert_order}...\")\n",
    "    transformed_images = []\n",
    "    for image in images:\n",
    "        transformed_image = transform_with_hilbert(image, hilbert_order)\n",
    "        transformed_images.append(transformed_image)\n",
    "    return np.array(transformed_images)\n",
    "\n",
    "# Load Dataset \n",
    "def load_dataset(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(image_size, image_size))\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize images\n",
    "                data.append(img)\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "# Load your dataset\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")\n",
    "images, labels, classes = load_dataset(main_dir, image_size)\n",
    "\n",
    "# Transform the dataset\n",
    "transformed_images = transform_dataset(images, hilbert_order)\n",
    "\n",
    "# Reshape to match CNN input (add channel dimension for grayscale)\n",
    "transformed_images = transformed_images.reshape(-1, grid_size, grid_size, 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "labels = to_categorical(labels, num_classes=len(classes))\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(transformed_images, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123)\n",
    "\n",
    "# CNN Model\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and Compile Model\n",
    "input_shape = (grid_size, grid_size, 1)\n",
    "cnn_model = create_cnn(input_shape, num_classes)\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "history = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vit ca. 15% accuray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 163ms/step - accuracy: 0.0992 - loss: 2.4894 - val_accuracy: 0.1187 - val_loss: 2.2407 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 168ms/step - accuracy: 0.1175 - loss: 2.2348 - val_accuracy: 0.1270 - val_loss: 2.2310 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 172ms/step - accuracy: 0.1240 - loss: 2.2214 - val_accuracy: 0.1322 - val_loss: 2.2127 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 168ms/step - accuracy: 0.1290 - loss: 2.2127 - val_accuracy: 0.1319 - val_loss: 2.2012 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 169ms/step - accuracy: 0.1295 - loss: 2.2087 - val_accuracy: 0.1318 - val_loss: 2.1943 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 171ms/step - accuracy: 0.1345 - loss: 2.2011 - val_accuracy: 0.1430 - val_loss: 2.1922 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 166ms/step - accuracy: 0.1349 - loss: 2.1973 - val_accuracy: 0.1444 - val_loss: 2.1875 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 165ms/step - accuracy: 0.1396 - loss: 2.1924 - val_accuracy: 0.1425 - val_loss: 2.1761 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 163ms/step - accuracy: 0.1424 - loss: 2.1801 - val_accuracy: 0.1532 - val_loss: 2.1678 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 158ms/step - accuracy: 0.1373 - loss: 2.1748 - val_accuracy: 0.1443 - val_loss: 2.1684 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 159ms/step - accuracy: 0.1405 - loss: 2.1719 - val_accuracy: 0.1448 - val_loss: 2.1676 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 160ms/step - accuracy: 0.1450 - loss: 2.1706 - val_accuracy: 0.1543 - val_loss: 2.1638 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 163ms/step - accuracy: 0.1437 - loss: 2.1717 - val_accuracy: 0.1519 - val_loss: 2.1709 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 165ms/step - accuracy: 0.1469 - loss: 2.1641 - val_accuracy: 0.1538 - val_loss: 2.1608 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 163ms/step - accuracy: 0.1431 - loss: 2.1652 - val_accuracy: 0.1497 - val_loss: 2.1644 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m 737/1575\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 152ms/step - accuracy: 0.1476 - loss: 2.1608"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 114\u001b[0m\n\u001b[1;32m    107\u001b[0m vit_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    108\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate),\n\u001b[1;32m    109\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    110\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Train the ViT model\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    125\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m vit_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/myenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Parameters\n",
    "image_size = 64\n",
    "num_classes = 18\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(image_size, image_size))\n",
    "                img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                data.append(img)\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "# Load dataset\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "data = data / 255.0  # Normalize to [0, 1]\n",
    "labels = to_categorical(labels, num_classes=len(classes))\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=123)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Vision Transformer Configuration\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def create_vit_classifier(image_size, num_classes):\n",
    "    patch_size = 8\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    projection_dim = 64\n",
    "    transformer_units = [projection_dim * 2, projection_dim]\n",
    "    num_heads = 4\n",
    "    transformer_layers = 8\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
    "    patches = layers.Conv2D(\n",
    "        filters=projection_dim,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        padding=\"VALID\"\n",
    "    )(inputs)\n",
    "\n",
    "    flat_patches = layers.Reshape((-1, projection_dim))(patches)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    positional_encoding = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=projection_dim\n",
    "    )(positions)\n",
    "    encoded_patches = flat_patches + positional_encoding\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=dropout_rate)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile the ViT model\n",
    "vit_model = create_vit_classifier(image_size=image_size, num_classes=num_classes)\n",
    "vit_model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the ViT model\n",
    "history = vit_model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = vit_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train accuracy 16% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hy/b81lcpw16vngs5l3k2jfppy00000gn/T/ipykernel_58667/2988657378.py:15: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,234</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │         \u001b[38;5;34m9,234\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,923,090</span> (11.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,923,090\u001b[0m (11.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665,106</span> (2.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m665,106\u001b[0m (2.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - accuracy: 0.1148 - loss: 2.6335 - val_accuracy: 0.1401 - val_loss: 2.1957 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.1345 - loss: 2.2514 - val_accuracy: 0.1420 - val_loss: 2.1773 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.1381 - loss: 2.2006 - val_accuracy: 0.1474 - val_loss: 2.1536 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - accuracy: 0.1430 - loss: 2.1721 - val_accuracy: 0.1499 - val_loss: 2.1552 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 32ms/step - accuracy: 0.1488 - loss: 2.1612 - val_accuracy: 0.1483 - val_loss: 2.1624 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 34ms/step - accuracy: 0.1465 - loss: 2.1564 - val_accuracy: 0.1488 - val_loss: 2.1428 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 32ms/step - accuracy: 0.1470 - loss: 2.1508 - val_accuracy: 0.1479 - val_loss: 2.1523 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.1481 - loss: 2.1446 - val_accuracy: 0.1541 - val_loss: 2.1488 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 32ms/step - accuracy: 0.1530 - loss: 2.1434 - val_accuracy: 0.1521 - val_loss: 2.1489 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.1532 - loss: 2.1368 - val_accuracy: 0.1469 - val_loss: 2.1453 - learning_rate: 1.0000e-04\n",
      "Epoch 1/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 93ms/step - accuracy: 0.1047 - loss: 3.0543 - val_accuracy: 0.1171 - val_loss: 2.2754 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 90ms/step - accuracy: 0.1267 - loss: 2.2561 - val_accuracy: 0.1247 - val_loss: 2.2513 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 92ms/step - accuracy: 0.1396 - loss: 2.2057 - val_accuracy: 0.1350 - val_loss: 2.1893 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 90ms/step - accuracy: 0.1407 - loss: 2.1778 - val_accuracy: 0.1399 - val_loss: 2.1679 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 87ms/step - accuracy: 0.1453 - loss: 2.1630 - val_accuracy: 0.1451 - val_loss: 2.1960 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 87ms/step - accuracy: 0.1480 - loss: 2.1457 - val_accuracy: 0.1470 - val_loss: 2.1921 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 87ms/step - accuracy: 0.1533 - loss: 2.1371 - val_accuracy: 0.1518 - val_loss: 2.1966 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 90ms/step - accuracy: 0.1590 - loss: 2.1268 - val_accuracy: 0.1452 - val_loss: 2.2553 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1629 - loss: 2.1217\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 89ms/step - accuracy: 0.1629 - loss: 2.1217 - val_accuracy: 0.1396 - val_loss: 2.3275 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 90ms/step - accuracy: 0.1582 - loss: 2.1198 - val_accuracy: 0.1388 - val_loss: 2.3693 - learning_rate: 5.0000e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 89ms/step - accuracy: 0.1605 - loss: 2.1127 - val_accuracy: 0.1423 - val_loss: 2.4117 - learning_rate: 5.0000e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 90ms/step - accuracy: 0.1700 - loss: 2.1067 - val_accuracy: 0.1363 - val_loss: 2.4963 - learning_rate: 5.0000e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 89ms/step - accuracy: 0.1721 - loss: 2.1062 - val_accuracy: 0.1347 - val_loss: 2.5338 - learning_rate: 5.0000e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1689 - loss: 2.1019\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 88ms/step - accuracy: 0.1689 - loss: 2.1019 - val_accuracy: 0.1342 - val_loss: 2.5876 - learning_rate: 5.0000e-06\n",
      "225/225 - 5s - 21ms/step - accuracy: 0.1483 - loss: 2.1518\n",
      "Test accuracy: 14.83%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Parameters\n",
    "input_shape = (64, 64, 3)\n",
    "num_classes = 18\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Load the MobileNetV2 model, excluding the top layers\n",
    "base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the base model initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Reduce the spatial dimensions to a single vector\n",
    "    Dense(512, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.5),  # Regularization\n",
    "    Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the frozen model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "# Unfreeze the base model and fine-tune\n",
    "base_model.trainable = True\n",
    "\n",
    "# Re-compile with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "fine_tune_history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model 16% accuracy\n",
    "model.save('my_model4_16.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train accuracy 33% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,234</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │         \u001b[38;5;34m9,234\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,066</span> (4.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,290,066\u001b[0m (4.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,288,146</span> (4.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,288,146\u001b[0m (4.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> (7.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,920\u001b[0m (7.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Input Shape\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "# Build the Model\n",
    "def build_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate Model\n",
    "num_classes = 18\n",
    "model = build_model(input_shape, num_classes)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 447ms/step - accuracy: 0.1330 - loss: 2.8077 - val_accuracy: 0.1244 - val_loss: 3.1328 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 436ms/step - accuracy: 0.1511 - loss: 2.6103 - val_accuracy: 0.1433 - val_loss: 2.6992 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 447ms/step - accuracy: 0.1676 - loss: 2.4638 - val_accuracy: 0.1297 - val_loss: 2.9166 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 467ms/step - accuracy: 0.1777 - loss: 2.3739 - val_accuracy: 0.1357 - val_loss: 2.6290 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 449ms/step - accuracy: 0.1886 - loss: 2.2855 - val_accuracy: 0.1530 - val_loss: 2.5439 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 459ms/step - accuracy: 0.1958 - loss: 2.2256 - val_accuracy: 0.1799 - val_loss: 2.2760 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 458ms/step - accuracy: 0.2028 - loss: 2.1733 - val_accuracy: 0.1793 - val_loss: 2.2432 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m706s\u001b[0m 448ms/step - accuracy: 0.2132 - loss: 2.1332 - val_accuracy: 0.1772 - val_loss: 2.3446 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 437ms/step - accuracy: 0.2138 - loss: 2.1078 - val_accuracy: 0.2074 - val_loss: 2.1334 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m695s\u001b[0m 441ms/step - accuracy: 0.2245 - loss: 2.0716 - val_accuracy: 0.2066 - val_loss: 2.2662 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m704s\u001b[0m 447ms/step - accuracy: 0.2301 - loss: 2.0450 - val_accuracy: 0.2153 - val_loss: 2.1339 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 445ms/step - accuracy: 0.2310 - loss: 2.0214 - val_accuracy: 0.2110 - val_loss: 2.1507 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 444ms/step - accuracy: 0.2386 - loss: 2.0081 - val_accuracy: 0.2156 - val_loss: 2.1054 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 444ms/step - accuracy: 0.2417 - loss: 1.9890 - val_accuracy: 0.1960 - val_loss: 2.1412 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 444ms/step - accuracy: 0.2468 - loss: 1.9768 - val_accuracy: 0.2074 - val_loss: 2.1400 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 446ms/step - accuracy: 0.2554 - loss: 1.9573 - val_accuracy: 0.2059 - val_loss: 2.1561 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 449ms/step - accuracy: 0.2494 - loss: 1.9619 - val_accuracy: 0.2151 - val_loss: 2.0921 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 458ms/step - accuracy: 0.2542 - loss: 1.9413 - val_accuracy: 0.2178 - val_loss: 2.0751 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 444ms/step - accuracy: 0.2629 - loss: 1.9282 - val_accuracy: 0.2086 - val_loss: 2.1211 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 435ms/step - accuracy: 0.2622 - loss: 1.9232 - val_accuracy: 0.2013 - val_loss: 2.1846 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 436ms/step - accuracy: 0.2660 - loss: 1.9141 - val_accuracy: 0.2066 - val_loss: 2.1768 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 436ms/step - accuracy: 0.2751 - loss: 1.8985 - val_accuracy: 0.1590 - val_loss: 2.5342 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 436ms/step - accuracy: 0.2688 - loss: 1.8991 - val_accuracy: 0.2288 - val_loss: 2.0531 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 435ms/step - accuracy: 0.2767 - loss: 1.8876 - val_accuracy: 0.2358 - val_loss: 2.0345 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 435ms/step - accuracy: 0.2809 - loss: 1.8827 - val_accuracy: 0.2474 - val_loss: 1.9775 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 436ms/step - accuracy: 0.2807 - loss: 1.8733 - val_accuracy: 0.2258 - val_loss: 2.0390 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 435ms/step - accuracy: 0.2895 - loss: 1.8617 - val_accuracy: 0.2540 - val_loss: 1.9640 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 449ms/step - accuracy: 0.2898 - loss: 1.8534 - val_accuracy: 0.2535 - val_loss: 1.9875 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 454ms/step - accuracy: 0.2921 - loss: 1.8517 - val_accuracy: 0.2126 - val_loss: 2.1180 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 452ms/step - accuracy: 0.2954 - loss: 1.8463 - val_accuracy: 0.2485 - val_loss: 1.9822 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 453ms/step - accuracy: 0.3005 - loss: 1.8344 - val_accuracy: 0.2286 - val_loss: 2.0565 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 457ms/step - accuracy: 0.2994 - loss: 1.8352 - val_accuracy: 0.2253 - val_loss: 2.0613 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 445ms/step - accuracy: 0.3154 - loss: 1.7937 - val_accuracy: 0.2589 - val_loss: 1.9384 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 442ms/step - accuracy: 0.3186 - loss: 1.7835 - val_accuracy: 0.2562 - val_loss: 1.9541 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m742s\u001b[0m 471ms/step - accuracy: 0.3219 - loss: 1.7810 - val_accuracy: 0.2597 - val_loss: 1.9536 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 469ms/step - accuracy: 0.3257 - loss: 1.7730 - val_accuracy: 0.2660 - val_loss: 1.9131 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 468ms/step - accuracy: 0.3249 - loss: 1.7719 - val_accuracy: 0.2586 - val_loss: 1.9457 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m736s\u001b[0m 467ms/step - accuracy: 0.3274 - loss: 1.7722 - val_accuracy: 0.2607 - val_loss: 1.9357 - learning_rate: 1.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 461ms/step - accuracy: 0.3318 - loss: 1.7599 - val_accuracy: 0.2646 - val_loss: 1.9361 - learning_rate: 1.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 444ms/step - accuracy: 0.3333 - loss: 1.7553 - val_accuracy: 0.2534 - val_loss: 1.9635 - learning_rate: 1.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 445ms/step - accuracy: 0.3297 - loss: 1.7557 - val_accuracy: 0.2603 - val_loss: 1.9410 - learning_rate: 1.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 443ms/step - accuracy: 0.3294 - loss: 1.7581 - val_accuracy: 0.2598 - val_loss: 1.9440 - learning_rate: 1.0000e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 443ms/step - accuracy: 0.3326 - loss: 1.7557 - val_accuracy: 0.2603 - val_loss: 1.9455 - learning_rate: 1.0000e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 444ms/step - accuracy: 0.3352 - loss: 1.7508 - val_accuracy: 0.2590 - val_loss: 1.9477 - learning_rate: 1.0000e-06\n",
      "Epoch 45/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 443ms/step - accuracy: 0.3350 - loss: 1.7552 - val_accuracy: 0.2606 - val_loss: 1.9419 - learning_rate: 1.0000e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 435ms/step - accuracy: 0.3367 - loss: 1.7434 - val_accuracy: 0.2583 - val_loss: 1.9500 - learning_rate: 1.0000e-06\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 108ms/step - accuracy: 0.2744 - loss: 1.9088\n",
      "Test Accuracy: 0.27\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model 19% accuracy\n",
    "model.save('my_model4_33.keras')\n",
    "\n",
    "# Load des Modells\n",
    "#model = load_model('my_model2.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train accuracy 19% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/myenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Block 4\n",
    "        SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        SeparableConv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 128, 3)  # Match the image size in your dataset\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Build the CNN\n",
    "model = build_cnn(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "optimizer = Adam(learning_rate=0.005),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "\n",
    "\n",
    "\n",
    "    #loss='categorical_crossentropy',\n",
    "    #metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model without data augmentation\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[lr_scheduler],\n",
    "    epochs=50,  # Set number of epochs based on your requirements\n",
    "    batch_size=32,  # You can adjust the batch size\n",
    "    verbose=1,  # Displays progress during training\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model 19% accuracy\n",
    "model.save('my_model3.keras')\n",
    "\n",
    "# Load des Modells\n",
    "#model = load_model('my_model2.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        # Block 4\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 128, 3)  # Match the image size in your dataset\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Build the CNN\n",
    "model = build_cnn(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "optimizer = Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "\n",
    "\n",
    "\n",
    "    #loss='categorical_crossentropy',\n",
    "    #metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train accuracy 20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model --------------------\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.6),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 128, 3)  # Match the image size in your dataset\n",
    "input_shape = (image_size[0], image_size[1], 3)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Build the CNN\n",
    "model = build_cnn(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    #SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "#lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv/myenv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 38 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('my_model.keras')\n",
    "\n",
    "# Load des Modells\n",
    "model = load_model('my_model2.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.1529 - loss: 2.2070\n",
      "Test accuracy: 14.86%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
