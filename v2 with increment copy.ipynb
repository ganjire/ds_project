{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)  # For reproducibility\n",
    "n_series = 4000   # Number of time series per class\n",
    "n_points = 500    # Number of data points in each time series\n",
    "\n",
    "#output_dir = 'time_series_data'\n",
    "#os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_dir = os.path.expanduser(\"~/timeseries_data\")  # Creates the directory in your home folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR, MA and ARMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate AR, MA, or ARMA data with optional trend and seasonality\n",
    "def generate_time_series(model_type, order, n_points, trend_strength=0.1, seasonality_amplitude=0.5, seasonality_period=50, include_trend=True):\n",
    "   if model_type == 'AR':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -params]\n",
    "       ma = np.array([1])\n",
    "   elif model_type == 'MA':\n",
    "       params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.array([1])\n",
    "       ma = np.r_[1, params]\n",
    "   elif model_type == 'ARMA':\n",
    "       ar_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ma_params = np.random.uniform(-0.5, 0.5, size=order)\n",
    "       ar = np.r_[1, -ar_params]\n",
    "       ma = np.r_[1, ma_params]\n",
    "   else:\n",
    "       raise ValueError(\"Invalid model type. Use 'AR', 'MA', or 'ARMA'.\")\n",
    "   \n",
    "   \n",
    "      # Generate the process\n",
    "   process = sm.tsa.ArmaProcess(ar, ma)\n",
    "   data = process.generate_sample(nsample=n_points)\n",
    "   if include_trend:\n",
    "       trend = np.linspace(0, trend_strength * n_points, n_points)\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += trend + seasonality\n",
    "   else:\n",
    "       seasonality = seasonality_amplitude * np.sin(2 * np.pi * np.arange(n_points) / seasonality_period)\n",
    "       data += seasonality\n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for orders 1-3 with and without trend (Kernel crashed after 14 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loop to generate and save time series plots for each class\n",
    "model_types = ['AR', 'MA', 'ARMA']\n",
    "orders = [1, 2, 3]\n",
    "for model_type in model_types:\n",
    "   for order in orders:\n",
    "       for include_trend in [True, False]:\n",
    "           class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "           class_dir = os.path.join(output_dir, class_label)\n",
    "           os.makedirs(class_dir, exist_ok=True)\n",
    "           for i in range(n_series):\n",
    "               data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "               # Plotting the time series\n",
    "               plt.figure(figsize=(8, 4))\n",
    "               plt.plot(data)\n",
    "               plt.axis('off')  # Turn off axes for a clean image\n",
    "               plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "               plt.close()\n",
    "print(\"Time series generation completed. Time series are saved in the 'time_series_data' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating image for remaining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMA_1_without_trend: 4000 images\n",
      "ARMA_2_with_trend: 4000 images\n",
      "MA_3_with_trend: 4000 images\n",
      "ARMA_2_without_trend: 4000 images\n",
      "AR_3_without_trend: 4000 images\n",
      "MA_1_without_trend: 4000 images\n",
      "MA_1_with_trend: 4000 images\n",
      "AR_2_with_trend: 4000 images\n",
      "MA_2_without_trend: 4000 images\n",
      "AR_1_with_trend: 4000 images\n",
      "MA_2_with_trend: 4000 images\n",
      "MA_3_without_trend: 4000 images\n",
      "ARMA_3_with_trend: 4000 images\n",
      "AR_3_with_trend: 4000 images\n",
      "AR_2_without_trend: 4000 images\n",
      "ARMA_3_without_trend: 4000 images\n",
      "AR_1_without_trend: 4000 images\n",
      "ARMA_1_with_trend: 4000 images\n"
     ]
    }
   ],
   "source": [
    "# Adjusted classes for ARMA 2 and 3 with and without trend\n",
    "model_type = 'ARMA'\n",
    "orders = [2, 3]\n",
    "for order in orders:\n",
    "    for include_trend in [True, False]:\n",
    "        class_label = f'{model_type}_{order}_with_trend' if include_trend else f'{model_type}_{order}_without_trend'\n",
    "        class_dir = os.path.join(output_dir, class_label)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for i in range(n_series):\n",
    "            data = generate_time_series(model_type, order, n_points, include_trend=include_trend)\n",
    "            # Plotting the time series\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(data)\n",
    "            plt.axis('off')  # Turn off axes for a clean image\n",
    "            plt.savefig(os.path.join(class_dir, f'Series_{i+1}.png'), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "\n",
    "# Code to check the number of images in each folder\n",
    "folder_status = {}\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_status[folder_name] = len(os.listdir(folder_path))\n",
    "\n",
    "# Display folder status\n",
    "for class_label, image_count in folder_status.items():\n",
    "    print(f\"{class_label}: {image_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Global parameters\n",
    "image_size = (224, 224)  # Resize all images to this size ->>>>> later time when training model\n",
    "# batch_size = 32  # Adjust based on hardware capacity\n",
    "\n",
    "main_dir = os.path.expanduser(\"~/timeseries_data\")  # Path to your main directory with class subfolders\n",
    "\n",
    "\n",
    "def load_data(main_dir, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])  # Filter directories only\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # Mapping class names to indices\n",
    "\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(main_dir, cls)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if img_file.endswith(('.png', '.jpg', '.jpeg')):  # Ensure it's an image file\n",
    "                img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
    "                img = img.resize(image_size)  # Resize image\n",
    "                data.append(np.array(img))\n",
    "                labels.append(class_to_idx[cls])\n",
    "\n",
    "    return np.array(data), np.array(labels), classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping: {'ARMA_1_with_trend': 0, 'ARMA_1_without_trend': 1, 'ARMA_2_with_trend': 2, 'ARMA_2_without_trend': 3, 'ARMA_3_with_trend': 4, 'ARMA_3_without_trend': 5, 'AR_1_with_trend': 6, 'AR_1_without_trend': 7, 'AR_2_with_trend': 8, 'AR_2_without_trend': 9, 'AR_3_with_trend': 10, 'AR_3_without_trend': 11, 'MA_1_with_trend': 12, 'MA_1_without_trend': 13, 'MA_2_with_trend': 14, 'MA_2_without_trend': 15, 'MA_3_with_trend': 16, 'MA_3_without_trend': 17}\n"
     ]
    }
   ],
   "source": [
    "# Generate a mapping for classes\n",
    "classes = sorted([cls for cls in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, cls))])\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}  # {'AR_1_with_trend': 0, 'MA_1_without_trend': 1, ...}\n",
    "print(f\"Class to index mapping: {class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Classes: ['ARMA_1_with_trend', 'ARMA_1_without_trend', 'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', 'ARMA_3_without_trend', 'AR_1_with_trend', 'AR_1_without_trend', 'AR_2_with_trend', 'AR_2_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_1_with_trend', 'MA_1_without_trend', 'MA_2_with_trend', 'MA_2_without_trend', 'MA_3_with_trend', 'MA_3_without_trend']\n",
      "Number of classes: 18\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data, labels, classes = load_data(main_dir, image_size)\n",
    "\n",
    "# Normalize data\n",
    "data = data / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Print class information\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Subset of classes to start with (e.g., 8 representative classes)\n",
    "subset_classes = ['AR_1_with_trend', 'AR_2_without_trend', \n",
    "                  'MA_2_with_trend', 'MA_3_without_trend',\n",
    "                  'ARMA_1_with_trend']\n",
    "\n",
    "# Map subset classes to indices\n",
    "subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "\n",
    "# Filter data and labels for the subset\n",
    "subset_mask = np.isin(labels, subset_indices)\n",
    "data_subset = data[subset_mask]\n",
    "labels_subset = labels[subset_mask]\n",
    "\n",
    "# Reindex labels for the subset\n",
    "labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "\n",
    "data_subset = data_subset.astype('float32')\n",
    "\n",
    "# 'MA_3_without_trend', 'AR_3_with_trend', 'ARMA_2_without_trend', 'ARMA_1_with_trend'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data subset shape: (20000, 224, 224, 3), size: 11484.38 MB\n",
      "Labels subset shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data subset shape: {data_subset.shape}, size: {data_subset.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"Labels subset shape: {labels_subset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "# Split indices instead of the whole dataset\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    np.arange(len(labels_subset)), test_size=0.3, stratify=labels_subset, random_state=42\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=1/3, stratify=labels_subset[temp_idx], random_state=42\n",
    ")\n",
    "\n",
    "# Custom DataGenerator that uses indices\n",
    "class IndexedDataGenerator(Sequence):\n",
    "    def __init__(self, data, labels, indices, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_data = self.data[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "# Use indices to create separate generators\n",
    "train_generator = IndexedDataGenerator(data_subset, labels_subset, train_idx, batch_size=32)\n",
    "val_generator = IndexedDataGenerator(data_subset, labels_subset, val_idx, batch_size=32)\n",
    "test_generator = IndexedDataGenerator(data_subset, labels_subset, test_idx, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.3980 - loss: 1.1213\n",
      "Epoch 1: val_accuracy improved from -inf to 0.44025, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 941ms/step - accuracy: 0.3980 - loss: 1.1212 - val_accuracy: 0.4403 - val_loss: 0.9981 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709ms/step - accuracy: 0.4296 - loss: 0.9796\n",
      "Epoch 2: val_accuracy improved from 0.44025 to 0.46600, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 915ms/step - accuracy: 0.4296 - loss: 0.9796 - val_accuracy: 0.4660 - val_loss: 0.9305 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734ms/step - accuracy: 0.4342 - loss: 0.9566\n",
      "Epoch 3: val_accuracy improved from 0.46600 to 0.48050, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 951ms/step - accuracy: 0.4342 - loss: 0.9566 - val_accuracy: 0.4805 - val_loss: 0.9207 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719ms/step - accuracy: 0.4532 - loss: 0.9425\n",
      "Epoch 4: val_accuracy improved from 0.48050 to 0.49375, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 923ms/step - accuracy: 0.4532 - loss: 0.9426 - val_accuracy: 0.4938 - val_loss: 0.9144 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731ms/step - accuracy: 0.4488 - loss: 0.9389\n",
      "Epoch 5: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 936ms/step - accuracy: 0.4488 - loss: 0.9389 - val_accuracy: 0.4852 - val_loss: 0.9114 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717ms/step - accuracy: 0.4583 - loss: 0.9286\n",
      "Epoch 6: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 923ms/step - accuracy: 0.4583 - loss: 0.9286 - val_accuracy: 0.4800 - val_loss: 0.9088 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - accuracy: 0.4654 - loss: 0.9246\n",
      "Epoch 7: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 914ms/step - accuracy: 0.4654 - loss: 0.9246 - val_accuracy: 0.4807 - val_loss: 0.9055 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708ms/step - accuracy: 0.4571 - loss: 0.9213\n",
      "Epoch 8: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 920ms/step - accuracy: 0.4571 - loss: 0.9212 - val_accuracy: 0.4855 - val_loss: 0.9016 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.4724 - loss: 0.9168\n",
      "Epoch 9: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 926ms/step - accuracy: 0.4724 - loss: 0.9168 - val_accuracy: 0.4793 - val_loss: 0.8939 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.4723 - loss: 0.9171\n",
      "Epoch 10: val_accuracy did not improve from 0.49375\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 921ms/step - accuracy: 0.4723 - loss: 0.9171 - val_accuracy: 0.4908 - val_loss: 0.8949 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688ms/step - accuracy: 0.4822 - loss: 0.9001\n",
      "Epoch 11: val_accuracy improved from 0.49375 to 0.49725, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 883ms/step - accuracy: 0.4822 - loss: 0.9001 - val_accuracy: 0.4972 - val_loss: 0.8891 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - accuracy: 0.4790 - loss: 0.9055\n",
      "Epoch 12: val_accuracy improved from 0.49725 to 0.50325, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 875ms/step - accuracy: 0.4790 - loss: 0.9055 - val_accuracy: 0.5033 - val_loss: 0.8879 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679ms/step - accuracy: 0.4822 - loss: 0.9017\n",
      "Epoch 13: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 868ms/step - accuracy: 0.4822 - loss: 0.9017 - val_accuracy: 0.4930 - val_loss: 0.8851 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - accuracy: 0.4957 - loss: 0.8895\n",
      "Epoch 14: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 856ms/step - accuracy: 0.4957 - loss: 0.8895 - val_accuracy: 0.4940 - val_loss: 0.8851 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668ms/step - accuracy: 0.4971 - loss: 0.8869\n",
      "Epoch 15: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 858ms/step - accuracy: 0.4971 - loss: 0.8869 - val_accuracy: 0.4978 - val_loss: 0.8875 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687ms/step - accuracy: 0.5066 - loss: 0.8845\n",
      "Epoch 16: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 876ms/step - accuracy: 0.5065 - loss: 0.8845 - val_accuracy: 0.4942 - val_loss: 0.8842 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - accuracy: 0.5028 - loss: 0.8836\n",
      "Epoch 17: val_accuracy did not improve from 0.50325\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 856ms/step - accuracy: 0.5028 - loss: 0.8837 - val_accuracy: 0.4990 - val_loss: 0.8790 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677ms/step - accuracy: 0.4885 - loss: 0.8938\n",
      "Epoch 18: val_accuracy improved from 0.50325 to 0.50525, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 871ms/step - accuracy: 0.4885 - loss: 0.8938 - val_accuracy: 0.5052 - val_loss: 0.8804 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682ms/step - accuracy: 0.5005 - loss: 0.8901\n",
      "Epoch 19: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 876ms/step - accuracy: 0.5005 - loss: 0.8901 - val_accuracy: 0.5035 - val_loss: 0.8724 - learning_rate: 1.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683ms/step - accuracy: 0.5067 - loss: 0.8813\n",
      "Epoch 20: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 877ms/step - accuracy: 0.5067 - loss: 0.8813 - val_accuracy: 0.5038 - val_loss: 0.8732 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Parameters\n",
    "IMAGE_SIZE = (224, 224, 3)  # Input size for ResNet50\n",
    "NUM_CLASSES = len(subset_classes)  # Number of classes in the subset\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "# Load the ResNet50 model with pretrained weights\n",
    "base_model = ResNet50(\n",
    "    input_shape=IMAGE_SIZE,\n",
    "    include_top=False,  # Exclude the final classification layers\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Pool the feature maps to a single vector\n",
    "x = BatchNormalization()(x)  # Normalize for stability\n",
    "x = Dense(512, activation=\"relu\")(x)  # Fully connected layer\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(NUM_CLASSES, activation=\"softmax\")(x)  # Final classification layer\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),  # Use a lower learning rate for transfer learning\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"subset_model.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model on the subset\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4176 - loss: 1.0157\n",
      "Epoch 1: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 2s/step - accuracy: 0.4176 - loss: 1.0157 - val_accuracy: 0.4375 - val_loss: 0.9349 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4503 - loss: 0.9345\n",
      "Epoch 2: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 2s/step - accuracy: 0.4503 - loss: 0.9345 - val_accuracy: 0.4415 - val_loss: 0.9331 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4747 - loss: 0.9176\n",
      "Epoch 3: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.4746 - loss: 0.9176 - val_accuracy: 0.4600 - val_loss: 0.9086 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4951 - loss: 0.8946\n",
      "Epoch 4: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 2s/step - accuracy: 0.4951 - loss: 0.8947 - val_accuracy: 0.4462 - val_loss: 0.9176 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4909 - loss: 0.8817\n",
      "Epoch 5: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m860s\u001b[0m 2s/step - accuracy: 0.4909 - loss: 0.8817 - val_accuracy: 0.4405 - val_loss: 0.9327 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5009 - loss: 0.8775\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 2s/step - accuracy: 0.5009 - loss: 0.8775 - val_accuracy: 0.4180 - val_loss: 0.9657 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5171 - loss: 0.8507\n",
      "Epoch 7: val_accuracy did not improve from 0.50525\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.5172 - loss: 0.8507 - val_accuracy: 0.4820 - val_loss: 0.8925 - learning_rate: 5.0000e-06\n",
      "Epoch 8/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5423 - loss: 0.8352\n",
      "Epoch 8: val_accuracy improved from 0.50525 to 0.50925, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.5422 - loss: 0.8352 - val_accuracy: 0.5092 - val_loss: 0.8573 - learning_rate: 5.0000e-06\n",
      "Epoch 9/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5521 - loss: 0.8245\n",
      "Epoch 9: val_accuracy improved from 0.50925 to 0.52225, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m865s\u001b[0m 2s/step - accuracy: 0.5521 - loss: 0.8245 - val_accuracy: 0.5222 - val_loss: 0.8633 - learning_rate: 5.0000e-06\n",
      "Epoch 10/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5683 - loss: 0.8043\n",
      "Epoch 10: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.5683 - loss: 0.8044 - val_accuracy: 0.5052 - val_loss: 0.8566 - learning_rate: 5.0000e-06\n",
      "Epoch 11/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5700 - loss: 0.7995\n",
      "Epoch 11: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.5700 - loss: 0.7995 - val_accuracy: 0.4940 - val_loss: 0.9390 - learning_rate: 5.0000e-06\n",
      "Epoch 12/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5836 - loss: 0.7823\n",
      "Epoch 12: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m861s\u001b[0m 2s/step - accuracy: 0.5836 - loss: 0.7823 - val_accuracy: 0.5120 - val_loss: 0.8580 - learning_rate: 5.0000e-06\n",
      "Epoch 13/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6003 - loss: 0.7733\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.52225\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m859s\u001b[0m 2s/step - accuracy: 0.6003 - loss: 0.7733 - val_accuracy: 0.5105 - val_loss: 0.9236 - learning_rate: 5.0000e-06\n",
      "Epoch 14/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6379 - loss: 0.7218\n",
      "Epoch 14: val_accuracy improved from 0.52225 to 0.52925, saving model to subset_model.keras\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.6379 - loss: 0.7218 - val_accuracy: 0.5293 - val_loss: 0.8920 - learning_rate: 2.5000e-06\n",
      "Epoch 15/15\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6464 - loss: 0.7027\n",
      "Epoch 15: val_accuracy did not improve from 0.52925\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 2s/step - accuracy: 0.6464 - loss: 0.7027 - val_accuracy: 0.5217 - val_loss: 0.9021 - learning_rate: 2.5000e-06\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze some layers in the base model for fine-tuning\n",
    "for layer in base_model.layers[-50:]:  # Unfreeze the last 50 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_finetune = model.fit(\n",
    "    train_generator,  # Use the same training data\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,  # Fine-tune for fewer epochs to avoid overfitting\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nor running cus new dataset function below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model(\"subset_model.keras\")  # Load your pre-trained model with the initial 5 classes\n",
    "\n",
    "def update_dataset(data, labels, class_to_idx, subset_classes, batch_size=32):\n",
    "    # Map subset classes to indices\n",
    "    subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "\n",
    "    # Filter data and labels for the subset\n",
    "    subset_mask = np.isin(labels, subset_indices)\n",
    "    data_subset = data[subset_mask]\n",
    "    labels_subset = labels[subset_mask]\n",
    "\n",
    "    # Reindex labels for the subset\n",
    "    labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "    # Split data into train, validation, and test indices\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        np.arange(len(labels_subset)), test_size=0.3, stratify=labels_subset, random_state=42\n",
    "    )\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx, test_size=1/3, stratify=labels_subset[temp_idx], random_state=42\n",
    "    )\n",
    "\n",
    "    # Create new generators\n",
    "    train_generator = IndexedDataGenerator(data_subset, labels_subset, train_idx, batch_size=batch_size)\n",
    "    val_generator = IndexedDataGenerator(data_subset, labels_subset, val_idx, batch_size=batch_size)\n",
    "\n",
    "    return train_generator, val_generator, data_subset, labels_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_model(model, num_classes, learning_rate=1e-5):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense\n",
    "\n",
    "    # Modify the output layer for the new number of classes\n",
    "    new_output = Dense(num_classes, activation=\"softmax\")(model.layers[-3].output)\n",
    "    updated_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    # Recompile the model with a smaller learning rate\n",
    "    updated_model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not running new function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "def incremental_training(\n",
    "    model,\n",
    "    data,\n",
    "    labels,\n",
    "    class_to_idx,\n",
    "    current_classes,\n",
    "    new_classes,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    learning_rate=1e-5,\n",
    "    checkpoint_name=\"incremental_model\"\n",
    "):\n",
    "    # Create a copy of current_classes and add new classes to it\n",
    "    updated_classes = current_classes + new_classes\n",
    "\n",
    "    # Update dataset and generators\n",
    "    train_generator, val_generator, _, _ = update_dataset(data, labels, class_to_idx, updated_classes, batch_size)\n",
    "\n",
    "    # Update the model\n",
    "    updated_model = update_model(model, len(updated_classes), learning_rate)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "    checkpoint = ModelCheckpoint(f\"{checkpoint_name}_{len(updated_classes)}_classes.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = updated_model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return updated_model, history, updated_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel crashed after the first increment -> model 8 classes saved tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the current subset of classes\n",
    "current_classes = ['AR_1_with_trend', 'AR_2_without_trend', 'MA_2_with_trend', 'MA_3_without_trend', 'ARMA_1_with_trend']\n",
    "\n",
    "# Define the new classes to add incrementally\n",
    "additional_classes = [\n",
    "    ['AR_1_without_trend', 'MA_1_with_trend', 'MA_1_without_trend'], # First increment\n",
    "    ['AR_2_with_trend', 'MA_2_without_trend', 'ARMA_1_without_trend'], # Second increment\n",
    "    ['AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend'], # Third increment\n",
    "    ['ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', 'ARMA_3_without_trend'] # Final increment (4 classes)\n",
    "]\n",
    "\n",
    "# Train the model incrementally\n",
    "for new_classes in additional_classes:\n",
    "    model, history, current_classes = incremental_training(\n",
    "        model=model,\n",
    "        data=data,\n",
    "        labels=labels,\n",
    "        class_to_idx=class_to_idx,\n",
    "        current_classes=current_classes,\n",
    "        new_classes=new_classes,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        learning_rate=1e-5,\n",
    "        checkpoint_name=\"incremental_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model after the first increment\n",
    "#model = load_model(\"incremental_model_8_classes.keras\")\n",
    "\n",
    "# Define the current classes from the first increment\n",
    "current_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend',\n",
    "    'MA_2_with_trend', 'MA_3_without_trend',\n",
    "    'ARMA_1_with_trend', 'AR_1_without_trend',\n",
    "    'MA_1_with_trend', 'MA_1_without_trend',\n",
    "    'AR_2_with_trend', 'MA_2_without_trend', 'ARMA_1_without_trend', \n",
    "    'AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining classes to train\n",
    "remaining_classes = [\n",
    "    #'AR_2_with_trend', 'MA_2_without_trend', 'ARMA_1_without_trend',  # Increment 1\n",
    "    #'AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend',       # Increment 2\n",
    "    'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', # Increment 3\n",
    "    'ARMA_3_without_trend'  # Increment 4\n",
    "]\n",
    "\n",
    "# Split into smaller increments\n",
    "increment_1 = remaining_classes[:3]  # First batch of 3 classes\n",
    "increment_2 = remaining_classes[3:6]  # Second batch of 3 classes\n",
    "increment_3 = remaining_classes[6:9]  # Third batch of 3 classes\n",
    "increment_4 = remaining_classes[9:]   # Fourth batch of 1 class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(data, labels, class_to_idx, remembered_classes, new_classes, batch_size=32):\n",
    "    # Combine remembered and new classes\n",
    "    subset_classes = remembered_classes + new_classes\n",
    "    subset_indices = [class_to_idx[cls] for cls in subset_classes]\n",
    "    subset_mask = np.isin(labels, subset_indices)\n",
    "    data_subset = data[subset_mask]\n",
    "    labels_subset = labels[subset_mask]\n",
    "    labels_subset = np.array([subset_indices.index(lbl) for lbl in labels_subset])\n",
    "\n",
    "    # Split data into train, validation, and test indices\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        np.arange(len(labels_subset)), test_size=0.3, stratify=labels_subset, random_state=42\n",
    "    )\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx, test_size=1/3, stratify=labels_subset[temp_idx], random_state=42\n",
    "    )\n",
    "\n",
    "    # Create new generators\n",
    "    train_generator = IndexedDataGenerator(data_subset, labels_subset, train_idx, batch_size=batch_size)\n",
    "    val_generator = IndexedDataGenerator(data_subset, labels_subset, val_idx, batch_size=batch_size)\n",
    "\n",
    "    return train_generator, val_generator, data_subset, labels_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, num_classes, learning_rate=1e-5):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "\n",
    "    # Modify the output layer for the new number of classes\n",
    "    x = model.layers[-3].output  # Access the layer before the final layer\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    new_output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    # Create the updated model\n",
    "    updated_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    # Recompile the model\n",
    "    updated_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return updated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not running alreday have saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3155 - loss: 7.2498\n",
      "Epoch 1: val_accuracy improved from -inf to 0.33542, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1172s\u001b[0m 2s/step - accuracy: 0.3155 - loss: 7.2454 - val_accuracy: 0.3354 - val_loss: 1.9726 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3435 - loss: 1.6941\n",
      "Epoch 2: val_accuracy did not improve from 0.33542\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1132s\u001b[0m 2s/step - accuracy: 0.3435 - loss: 1.6938 - val_accuracy: 0.3333 - val_loss: 3.0074 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3525 - loss: 1.3137\n",
      "Epoch 3: val_accuracy did not improve from 0.33542\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1224s\u001b[0m 2s/step - accuracy: 0.3525 - loss: 1.3137 - val_accuracy: 0.1667 - val_loss: 7.1886 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3482 - loss: 1.2848\n",
      "Epoch 4: val_accuracy improved from 0.33542 to 0.37688, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 2s/step - accuracy: 0.3482 - loss: 1.2848 - val_accuracy: 0.3769 - val_loss: 1.2810 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3637 - loss: 1.2781\n",
      "Epoch 5: val_accuracy did not improve from 0.37688\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 2s/step - accuracy: 0.3637 - loss: 1.2781 - val_accuracy: 0.3333 - val_loss: 1.5252 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3637 - loss: 1.2757\n",
      "Epoch 6: val_accuracy did not improve from 0.37688\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1128s\u001b[0m 2s/step - accuracy: 0.3637 - loss: 1.2757 - val_accuracy: 0.3381 - val_loss: 1.5886 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3703 - loss: 1.2553\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.37688\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 2s/step - accuracy: 0.3703 - loss: 1.2553 - val_accuracy: 0.3419 - val_loss: 1.4137 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3793 - loss: 1.2122\n",
      "Epoch 8: val_accuracy did not improve from 0.37688\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1127s\u001b[0m 2s/step - accuracy: 0.3793 - loss: 1.2121 - val_accuracy: 0.3373 - val_loss: 1.2119 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3766 - loss: 1.1875\n",
      "Epoch 9: val_accuracy did not improve from 0.37688\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1125s\u001b[0m 2s/step - accuracy: 0.3766 - loss: 1.1875 - val_accuracy: 0.3325 - val_loss: 2.0931 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3811 - loss: 1.1975\n",
      "Epoch 10: val_accuracy did not improve from 0.37688\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1128s\u001b[0m 2s/step - accuracy: 0.3810 - loss: 1.1975 - val_accuracy: 0.3656 - val_loss: 1.3207 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "source": [
    "# Train the model on increment 1\n",
    "new_classes = increment_1  # First batch of 3 new classes\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    class_to_idx=class_to_idx,\n",
    "    remembered_classes=current_classes[-3:],  # Remember the first 3 classes\n",
    "    new_classes=new_classes,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Update the model\n",
    "#model = update_model(model, num_classes=len(current_classes[:3]) + len(new_classes), learning_rate=1e-5)\n",
    "\n",
    "\n",
    "model = update_model(\n",
    "    model, \n",
    "    num_classes=len(current_classes[-3:]) + len(new_classes),  # 8 existing + 3 new = 11\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"incremental_model_{len(current_classes[-3:]) + len(new_classes)}_classes.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update current classes\n",
    "current_classes = (current_classes[-3:]) + new_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model saved after increment 1\n",
    "model = load_model(\"incremental_model_6_classes.keras\")  # Model from increment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, num_classes, learning_rate=1e-5):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "\n",
    "    # Modify the output layer for the new number of classes\n",
    "    x = model.layers[-3].output  # Access the layer before the final layer\n",
    "    x = BatchNormalization(name=\"batch_norm_update_1\")(x)  # Unique name for the layer\n",
    "    x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense_update_1\")(x)  # Unique name\n",
    "    x = Dropout(0.5, name=\"dropout_update_1\")(x)  # Unique name\n",
    "    x = BatchNormalization(name=\"batch_norm_update_2\")(x)  # Unique name\n",
    "    x = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense_update_2\")(x)  # Unique name\n",
    "    x = Dropout(0.5, name=\"dropout_update_2\")(x)  # Unique name\n",
    "    new_output = Dense(num_classes, activation=\"softmax\", name=\"dense_output_update\")(x)  # Unique name\n",
    "\n",
    "    # Create the updated model\n",
    "    updated_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    # Recompile the model\n",
    "    updated_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return updated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3446 - loss: 6.1893\n",
      "Epoch 1: val_accuracy improved from -inf to 0.33375, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1100s\u001b[0m 2s/step - accuracy: 0.3446 - loss: 6.1860 - val_accuracy: 0.3338 - val_loss: 3.1009 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4173 - loss: 1.8325\n",
      "Epoch 2: val_accuracy did not improve from 0.33375\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1069s\u001b[0m 2s/step - accuracy: 0.4173 - loss: 1.8321 - val_accuracy: 0.1444 - val_loss: 109.7172 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4444 - loss: 1.3133\n",
      "Epoch 3: val_accuracy improved from 0.33375 to 0.36125, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1138s\u001b[0m 2s/step - accuracy: 0.4444 - loss: 1.3132 - val_accuracy: 0.3613 - val_loss: 30.8598 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4444 - loss: 1.2923\n",
      "Epoch 4: val_accuracy did not improve from 0.36125\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1151s\u001b[0m 2s/step - accuracy: 0.4444 - loss: 1.2923 - val_accuracy: 0.3542 - val_loss: 1.4145 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4378 - loss: 1.2893\n",
      "Epoch 5: val_accuracy improved from 0.36125 to 0.39479, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1148s\u001b[0m 2s/step - accuracy: 0.4378 - loss: 1.2894 - val_accuracy: 0.3948 - val_loss: 1.4521 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4545 - loss: 1.2824\n",
      "Epoch 6: val_accuracy improved from 0.39479 to 0.40292, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1103s\u001b[0m 2s/step - accuracy: 0.4545 - loss: 1.2825 - val_accuracy: 0.4029 - val_loss: 1.5788 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4544 - loss: 1.3283\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.40292\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1080s\u001b[0m 2s/step - accuracy: 0.4544 - loss: 1.3282 - val_accuracy: 0.3519 - val_loss: 1.4680 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4668 - loss: 1.1965\n",
      "Epoch 8: val_accuracy improved from 0.40292 to 0.42646, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1079s\u001b[0m 2s/step - accuracy: 0.4668 - loss: 1.1965 - val_accuracy: 0.4265 - val_loss: 1.2175 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4885 - loss: 1.1461\n",
      "Epoch 9: val_accuracy improved from 0.42646 to 0.45146, saving model to incremental_model_6_classes.keras\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 2s/step - accuracy: 0.4885 - loss: 1.1461 - val_accuracy: 0.4515 - val_loss: 1.1469 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4996 - loss: 1.1166\n",
      "Epoch 10: val_accuracy did not improve from 0.45146\n",
      "\u001b[1m525/525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1049s\u001b[0m 2s/step - accuracy: 0.4996 - loss: 1.1167 - val_accuracy: 0.4079 - val_loss: 1.3137 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "source": [
    "# Use increment 2 for the second batch of new classes\n",
    "new_classes = increment_2  # ['AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend']\n",
    "\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    class_to_idx=class_to_idx,\n",
    "    remembered_classes=current_classes[-3:],  # Remember the first 3 classes\n",
    "    new_classes=new_classes,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Update the model\n",
    "model = update_model(\n",
    "    model, \n",
    "    num_classes=len(current_classes[-3:]) + len(new_classes),  # 8 existing + 3 new = 11\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"incremental_model_{len(current_classes[-3:]) + len(new_classes)}_classes.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update current classes\n",
    "current_classes = (current_classes[-3:]) + new_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model saved after increment 2\n",
    "model = load_model(\"incremental_model_6_classes.keras\")  # Model from increment 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, num_classes, learning_rate=1e-5):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "\n",
    "    # Modify the output layer for the new number of classes\n",
    "    x = model.layers[-3].output  # Access the layer before the final layer\n",
    "    x = BatchNormalization(name=\"batch_norm_update_3_1\")(x)  # Unique name for the layer\n",
    "    x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense_update_3_1\")(x)  # Unique name\n",
    "    x = Dropout(0.5, name=\"dropout_update_3_1\")(x)  # Unique name\n",
    "    x = BatchNormalization(name=\"batch_norm_update_3_2\")(x)  # Unique name\n",
    "    x = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense_update_3_2\")(x)  # Unique name\n",
    "    x = Dropout(0.6, name=\"dropout_update_3_2\")(x)  # Unique name\n",
    "    new_output = Dense(num_classes, activation=\"softmax\", name=\"dense_output_update_3\")(x)  # Unique name\n",
    "\n",
    "    # Create the updated model\n",
    "    updated_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    # Recompile the model\n",
    "    updated_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return updated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6651 - loss: 5.6487\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69792, saving model to incremental_model_3_classes.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 2s/step - accuracy: 0.6652 - loss: 5.6432 - val_accuracy: 0.6979 - val_loss: 2.2799 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6886 - loss: 1.7692\n",
      "Epoch 2: val_accuracy did not improve from 0.69792\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 2s/step - accuracy: 0.6886 - loss: 1.7681 - val_accuracy: 0.4079 - val_loss: 2.2568 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7238 - loss: 1.0279\n",
      "Epoch 3: val_accuracy did not improve from 0.69792\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 2s/step - accuracy: 0.7238 - loss: 1.0276 - val_accuracy: 0.6550 - val_loss: 1.0346 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7157 - loss: 0.8560\n",
      "Epoch 4: val_accuracy improved from 0.69792 to 0.69958, saving model to incremental_model_3_classes.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 2s/step - accuracy: 0.7157 - loss: 0.8560 - val_accuracy: 0.6996 - val_loss: 0.9695 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7106 - loss: 0.9458\n",
      "Epoch 5: val_accuracy improved from 0.69958 to 0.73833, saving model to incremental_model_3_classes.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 2s/step - accuracy: 0.7106 - loss: 0.9457 - val_accuracy: 0.7383 - val_loss: 0.8924 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7136 - loss: 0.9078\n",
      "Epoch 6: val_accuracy did not improve from 0.73833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 2s/step - accuracy: 0.7136 - loss: 0.9078 - val_accuracy: 0.6979 - val_loss: 0.8648 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7136 - loss: 0.9239\n",
      "Epoch 7: val_accuracy did not improve from 0.73833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 2s/step - accuracy: 0.7137 - loss: 0.9238 - val_accuracy: 0.6425 - val_loss: 1.2695 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7184 - loss: 0.8308\n",
      "Epoch 8: val_accuracy did not improve from 0.73833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2s/step - accuracy: 0.7184 - loss: 0.8308 - val_accuracy: 0.6679 - val_loss: 1.0131 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7226 - loss: 0.9083\n",
      "Epoch 9: val_accuracy did not improve from 0.73833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 2s/step - accuracy: 0.7226 - loss: 0.9082 - val_accuracy: 0.7204 - val_loss: 0.8646 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7292 - loss: 0.8419\n",
      "Epoch 10: val_accuracy did not improve from 0.73833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 2s/step - accuracy: 0.7292 - loss: 0.8420 - val_accuracy: 0.6900 - val_loss: 0.8450 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "# Use increment 3 for the second batch of new classes\n",
    "new_classes = increment_3 \n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    class_to_idx=class_to_idx,\n",
    "    remembered_classes=current_classes[-3:],  # Remember the first 3 classes\n",
    "    new_classes=new_classes,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Update the model\n",
    "model = update_model(\n",
    "    model, \n",
    "    num_classes=len(current_classes[-3:]) + len(new_classes), \n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"incremental_model_{len(current_classes[-3:]) + len(new_classes)}_classes.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update current classes\n",
    "current_classes = (current_classes[-3:]) + new_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model saved after increment 3\n",
    "model = load_model(\"incremental_model_3_classes.keras\")  # Model from increment 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, num_classes, learning_rate=1e-5):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "\n",
    "    # Modify the output layer for the new number of classes\n",
    "    x = model.layers[-3].output  # Access the layer before the final layer\n",
    "    x = BatchNormalization(name=\"batch_norm_update_4_1\")(x)  # Unique name for the layer\n",
    "    x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense_update_4_1\")(x)  # Unique name\n",
    "    x = Dropout(0.5, name=\"dropout_update_4_1\")(x)  # Unique name\n",
    "    x = BatchNormalization(name=\"batch_norm_update_4_2\")(x)  # Unique name\n",
    "    x = Dense(256, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense_update_4_2\")(x)  # Unique name\n",
    "    x = Dropout(0.6, name=\"dropout_update_4_2\")(x)  # Unique name\n",
    "    new_output = Dense(num_classes, activation=\"softmax\", name=\"dense_output_update_4\")(x)  # Unique name\n",
    "\n",
    "    # Create the updated model\n",
    "    updated_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    # Recompile the model\n",
    "    updated_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return updated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccaganjineh/myenv_2/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6708 - loss: 6.2032\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71833, saving model to incremental_model_3_classes.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 2s/step - accuracy: 0.6708 - loss: 6.1973 - val_accuracy: 0.7183 - val_loss: 2.4992 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6998 - loss: 2.0560\n",
      "Epoch 2: val_accuracy did not improve from 0.71833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 2s/step - accuracy: 0.6998 - loss: 2.0547 - val_accuracy: 0.7150 - val_loss: 1.1826 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6984 - loss: 1.1828\n",
      "Epoch 3: val_accuracy did not improve from 0.71833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 2s/step - accuracy: 0.6984 - loss: 1.1827 - val_accuracy: 0.6821 - val_loss: 0.9673 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7057 - loss: 1.2019\n",
      "Epoch 4: val_accuracy did not improve from 0.71833\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 2s/step - accuracy: 0.7057 - loss: 1.2016 - val_accuracy: 0.3533 - val_loss: 1.3208 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7148 - loss: 1.0199\n",
      "Epoch 5: val_accuracy improved from 0.71833 to 0.72625, saving model to incremental_model_3_classes.keras\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 2s/step - accuracy: 0.7148 - loss: 1.0204 - val_accuracy: 0.7262 - val_loss: 1.1303 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7057 - loss: 1.0576\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.72625\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 2s/step - accuracy: 0.7056 - loss: 1.0576 - val_accuracy: 0.3333 - val_loss: 2.2835 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6661 - loss: 0.9989\n",
      "Epoch 7: val_accuracy did not improve from 0.72625\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 2s/step - accuracy: 0.6661 - loss: 0.9987 - val_accuracy: 0.6513 - val_loss: 1.0534 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6639 - loss: 0.8948\n",
      "Epoch 8: val_accuracy did not improve from 0.72625\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 2s/step - accuracy: 0.6639 - loss: 0.8946 - val_accuracy: 0.6667 - val_loss: 0.7543 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6745 - loss: 0.8161\n",
      "Epoch 9: val_accuracy did not improve from 0.72625\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 2s/step - accuracy: 0.6745 - loss: 0.8161 - val_accuracy: 0.6712 - val_loss: 0.8155 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6505 - loss: 0.9553\n",
      "Epoch 10: val_accuracy did not improve from 0.72625\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 2s/step - accuracy: 0.6505 - loss: 0.9552 - val_accuracy: 0.6704 - val_loss: 0.8586 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "source": [
    "# Use increment 4 for the second batch of new classes\n",
    "new_classes = increment_4\n",
    "\n",
    "# Update dataset\n",
    "train_generator, val_generator, _, _ = update_dataset(\n",
    "    data=data,\n",
    "    labels=labels,\n",
    "    class_to_idx=class_to_idx,\n",
    "    remembered_classes=current_classes[-3:],  # Remember the first 3 classes\n",
    "    new_classes=new_classes,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Update the model\n",
    "model = update_model(\n",
    "    model, \n",
    "    num_classes=len(current_classes[-3:]) + len(new_classes), \n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "# Define checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"incremental_model_{len(current_classes[-3:]) + len(new_classes)}_classes.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Update current classes\n",
    "current_classes = (current_classes[-3:]) + new_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 705ms/step - accuracy: 0.3463 - loss: 1.4209\n",
      "Test Loss: 1.3699865341186523\n",
      "Test Accuracy: 0.3605000078678131\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model (e.g., from the last increment) i think with 11?\n",
    "model = load_model(\"incremental_model_8_classes.keras\")  # Replace with the correct filename\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was trained on 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"incremental_model_8_classes.keras\")\n",
    "\n",
    "# Check the number of output classes\n",
    "num_classes = model.output_shape[-1]\n",
    "print(f\"The model was trained on {num_classes} classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was trained on the following classes: ['ARMA_1_with_trend', 'ARMA_1_without_trend', 'ARMA_2_with_trend', 'ARMA_2_without_trend', 'ARMA_3_with_trend', 'ARMA_3_without_trend', 'AR_1_with_trend', 'AR_1_without_trend', 'AR_2_with_trend', 'AR_2_without_trend', 'AR_3_with_trend', 'AR_3_without_trend', 'MA_1_with_trend', 'MA_1_without_trend', 'MA_2_with_trend', 'MA_2_without_trend', 'MA_3_with_trend', 'MA_3_without_trend']\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct class names using `class_to_idx`\n",
    "# Example: class_to_idx = {'class1': 0, 'class2': 1, 'class3': 2}\n",
    "trained_classes = [class_name for class_name, idx in sorted(class_to_idx.items(), key=lambda x: x[1])]\n",
    "print(f\"The model was trained on the following classes: {trained_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was trained on these classes: ['AR_3_with_trend', 'AR_3_without_trend', 'MA_3_with_trend']\n"
     ]
    }
   ],
   "source": [
    "# Assuming `current_classes` was updated after each increment\n",
    "print(f\"The model was trained on these classes: {current_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the previously saved model\n",
    "model = load_model(\"incremental_model_8_classes.keras\")  # Starting from the first increment (8 classes)\n",
    "\n",
    "# Define remembered classes\n",
    "remembered_classes = [\n",
    "    'AR_1_with_trend', 'AR_2_without_trend', \n",
    "    'MA_2_with_trend', 'MA_3_without_trend', \n",
    "    'ARMA_1_with_trend', 'AR_1_without_trend', \n",
    "    'MA_1_with_trend', 'MA_1_without_trend'\n",
    "]\n",
    "\n",
    "# Train each increment\n",
    "increments = [increment_1, increment_2, increment_3]\n",
    "for i, new_classes in enumerate(increments):\n",
    "    # Update the dataset for the current increment\n",
    "    train_generator, val_generator, _, _ = update_dataset(\n",
    "        data=data,\n",
    "        labels=labels,\n",
    "        class_to_idx=class_to_idx,\n",
    "        remembered_classes=remembered_classes[:5],  # Keep a subset of remembered classes (e.g., 5)\n",
    "        new_classes=new_classes,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Update the model for the new total number of classes\n",
    "    model = update_model(model, num_classes=len(remembered_classes[:5]) + len(new_classes), learning_rate=1e-5)\n",
    "    \n",
    "    # Define the checkpoint for this increment\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"incremental_model_{len(remembered_classes[:5]) + len(new_classes)}_classes.keras\", \n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=10,\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Update the remembered classes\n",
    "    remembered_classes += new_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('model_subset.keras')  # Use the correct file path and format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reload the full dataset\n",
    "data_full = data.astype('float32')  # Use the original dataset\n",
    "labels_full = labels  # Labels for all classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full class mapping (already created earlier)\n",
    "print(f\"All classes: {classes}\")\n",
    "print(f\"Number of classes: {len(classes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('model_subset5.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Train the model on the subset\n",
    "history_subset = model_subset.fit(train_generator,\n",
    "                                  validation_data=val_generator,\n",
    "                                  epochs=20,  # Adjust as needed\n",
    "                                  callbacks=[checkpoint, reduce_lr],\n",
    "                                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
